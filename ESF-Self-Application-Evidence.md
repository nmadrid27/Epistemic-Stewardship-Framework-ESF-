---
title: "Building ESF with ESF: Self-Application Evidence"
author: Nathan Madrid
date: 2026-02-19
type: evidence-document
epistemic-weight: high
tags: [esf, self-application, methodology]
---

# Building ESF with ESF

## How the Framework Was Used to Construct Its Own Documents

---

## Why This Document Exists

If a framework for maintaining intellectual ownership in AI-assisted work cannot demonstrate its own principles during its own construction, it fails its most basic credibility test. ESF was built through extensive human-AI collaboration across 7 phases and 8+ sessions. This document maps the actual human-AI division of labor, the evidence of intellectual ownership at each stage, and where the framework's own constructs were applied, tested, and sometimes revised through the process.

---

## Intellectual Provenance: Practice First, Literature Second

ESF's origin story is critical to understanding its claims and its authorship.

**The constructs came from practice, not from the literature.** Nathan Madrid developed the core mechanisms that became ESF (the pre-drafting authority document, the recurring checkpoint questions, the epistemic weight classification, the phased workflow with gates) through his own operational work integrating AI into curriculum development, assessment design, and institutional reporting at the Savannah College of Art and Design (SCAD). These mechanisms were built iteratively over months of daily AI-assisted practice across multiple courses (Foundation Studies, Applied AI, Professional Studies) and administrative responsibilities (Associate Chair duties, accreditation work, faculty development).

**The development sequence was:**

1. **Problem encountered in practice.** Nathan found that AI-assisted drafts drifted from his intent, that general "review your work" advice was insufficient, that different content types demanded different levels of human engagement, and that students needed a fundamentally different process than faculty.
2. **Solution built and tested.** Each construct was developed as a practical response: the Directive Memo to anchor intent, the Five Questions to structure review, Content Epistemic Weight to calibrate effort, the two-level architecture to serve different audiences.
3. **System operationalized in vault infrastructure.** The constructs were codified into Nathan's Obsidian vault as reference documents, skill definitions, agent instructions, and templates. This was a working system used daily, not a theoretical framework.
4. **Literature review conducted.** 31 scholarly sources across 8 research areas were identified and verified. The review confirmed that the constructs address documented problems (epistemic agency erosion, metacognitive failure in AI oversight, the faculty-facing gap) and that no existing framework provides the specific mechanisms Nathan had already built.
5. **Framework formalized for external audiences.** The operational system was generalized, stripped of SCAD-specific dependencies, grounded in the verified literature, and structured as a publishable framework with implementation guides.

**What the literature provided:** Validation that the problems ESF addresses are real and documented. Scholarly vocabulary for describing the constructs (e.g., Clark and Chalmers's "active endorsement" maps precisely to what the Directive Memo operationalizes). Evidence that the gaps ESF fills are genuine (e.g., Zawacki-Richter et al.'s 2019 identification of the faculty-facing gap remains open). Positioning against existing frameworks (AIAS, AID, ED-AI Lit, UNESCO) to show complementarity.

**What the literature did not provide:** The constructs themselves. The Directive Memo, Five Questions, Content Epistemic Weight, Human Validation Gates, the phased workflow, the Disclosure Protocol, and the Framework Evolution Protocol all originated from Nathan's practice. The two-level architecture, the most significant structural decision in the framework, was Nathan's judgment call based on his experience designing for both faculty production and student learning. No source in the literature proposes these specific mechanisms.

**Why this matters for credit and authorship:** AI assisted extensively with drafting, organizing, and cross-referencing during the formalization process (see the ratio table below). But the intellectual architecture (what ESF argues, what constructs it includes, how they relate, and what problems they solve) is Nathan Madrid's contribution. The framework exists because he built it in practice, recognized its broader applicability, and directed its formalization into a publishable form. The AI's contribution was production assistance within parameters Nathan established. The framework's intellectual content is his.

---

## The Self-Application Protocol

Before any ESF document was drafted, the framework's own protocol was followed:

1. **Directive Memo** written for Phase 1 (the Framework Document) establishing Nathan's intellectual position on what ESF should argue, how it should be structured, and what was non-negotiable
2. **Content Epistemic Weight** classified as HIGH for all framework documents (original scholarship establishing new constructs)
3. **Human Validation Gates** applied at every phase transition, with the Phase 1 gate producing the Gate Verification Record mechanism (which was then added to the framework itself)
4. **Five Questions** applied at each gate
5. **Disclosure statements** included on every deliverable

---

## Human-AI Division of Labor

### What Nathan Decided (Human Authority)

These decisions originated with the human and were not delegated to AI at any point:

| Decision | Evidence | Phase |
|----------|----------|-------|
| Framework name ("Epistemic Stewardship Framework") | Selected from options; confirmed before Phase 1 | Pre-1 |
| Core argument: epistemic ownership as the central problem, not compliance | Directive Memo, Phase 1 | 1 |
| Seven constructs and their definitions | Defined in Directive Memo; AI structured but did not select | 1 |
| Gate Verification Record mechanism | Nathan identified the gap during Phase 1 gate review: "gates need evidence of genuine engagement, not just a yes" | 1 (gate) |
| Glossary addition | Nathan requested mid-session; not in original plan | 1 |
| Two-level architecture decision | Nathan directed the structural split after reviewing v1.0 deliverables | Post-Phase 5 |
| Level 2 process names (Inquire, Position, Explore, Make, Reflect) | Nathan approved from options; AI proposed based on design thinking parallel | Post-Phase 5 |
| Human-first sequencing (AI excluded from student Phases 1-2) | Nathan confirmed as non-negotiable based on teaching experience | Post-Phase 5 |
| Counterargument selection | Nathan reviewed; AI proposed strongest plausible objections | 2 |
| Comparison Matrix framing (complementarity, not competition) | Nathan directed; AI implemented | 2 |
| Faculty Guide discipline examples (Biology, History, Business) | Nathan selected to ensure non-art/design coverage | 3 |
| Student guide written *to* students, not *about* them | Nathan directed second-person voice and agency framing | 4 |
| "Three options when no" (revise, reduce, be transparent) for students | Nathan directed; diverges from faculty halt condition | 4 |
| Mitigating factors clause for honest self-reporting | Nathan directed based on integrity policy experience | 5 |
| Accreditation crosswalk scope (SACSCOC, HLC, NASAD, ABET) | Nathan selected based on institutional relevance | 5 |
| Executive Summary format and worked example choice | Nathan directed for meeting use | Post-Phase 6 |

### What AI Drafted (AI-Assisted Production)

These were produced by AI within parameters Nathan established:

| Task | AI Role | Human Oversight |
|------|---------|----------------|
| Directive Memo initial draft | AI drafted from existing vault BMAD source files | Nathan accepted without revision (AI draft captured his position accurately) |
| Framework Document prose (~10,500 words) | AI drafted section by section within architectural plan | Nathan reviewed in IDE during and after drafting |
| Literature Review concept clusters (B.1-B.8) | AI organized sources into clusters with analysis | Nathan verified source representations; directed B.8 addition |
| Novel Contributions counterarguments | AI proposed strongest plausible objections | Nathan reviewed for authenticity ("would a reviewer actually say this?") |
| Comparison Matrix tables and analysis | AI constructed comparisons from framework knowledge | Nathan verified fairness to competing frameworks |
| Faculty Guide worked examples | AI drafted discipline-specific scenarios | Flagged for Nathan's disciplinary verification |
| Student Guide engagement levels | AI structured progression with readiness indicators | Nathan verified against teaching experience |
| Templates (6 total) | AI drafted from ESF constructs + vault template patterns | Nathan reviewed for institutional usability |
| Institutional Guide rollout model | AI drafted 3-phase model with success criteria | Nathan evaluated against institutional experience |
| Accreditation Crosswalk standard mappings | AI mapped from knowledge of accreditor standards | **HIGH VERIFICATION PRIORITY** flagged; needs independent verification |
| Vault infrastructure rename (13 files) | AI performed contextual find-and-replace | Nathan verified via grep that no active files retained old terminology |

### What Was Negotiated (Co-Created)

These emerged through back-and-forth between human direction and AI contribution:

| Item | Process |
|------|---------|
| **Framework Evolution Protocol as core construct** | Original plan placed it as an appendix. AI argued it deserved construct-level status given ESF's emphasis on adaptability. Nathan agreed. |
| **Content Epistemic Weight: 3 tiers vs. 5** | AI presented tradeoffs (AIAS uses 5). Nathan chose 3 for practical adoption, with acknowledgment in counterarguments. |
| **Gate Verification Record** | Nathan identified the need ("gates need evidence"). AI designed the mechanism. Nathan approved and directed its addition to the framework itself. |
| **Student Directive Memo simplification** | AI proposed 3 elements (from faculty's 6). Nathan confirmed: "compression revealed core function: force yourself to have a position." |
| **Kept/revised/rejected structure** | AI proposed for Student Reflection template. Nathan recognized it reframes evidence from "did you use AI?" to "what did you do with AI's output?" |
| **Two-level architecture propagation** | Nathan decided the architecture. AI proposed the propagation sequence (Student Guide first, then Faculty Guide, Literature Review, Novel Contributions, Comparison Matrix). Nathan directed. |

---

## Evidence of Intellectual Ownership at Each Phase

### Phase 1: Framework Document

- **Directive Memo**: Written before any document drafting. Established thesis, 7 constructs, emphasis (epistemic ownership over compliance), voice (direct, scholarly), non-negotiables.
- **Gate outcome**: Nathan reviewed document in IDE. Challenged the gate mechanism ("where's the evidence of genuine engagement?"). This challenge produced the Gate Verification Record, which was added to the framework. The gate itself improved the framework.
- **Five Questions applied**:
  - Can I defend this? Yes. Nathan can explain every construct and its scholarly basis.
  - Is this mine? Yes. The argument, constructs, and architecture originate from Nathan's operational practice.
  - Did I verify? Partially. 18 sources verified at plan level; exact bibliographic details flagged for pre-publication confirmation.
  - Would I teach this? Yes. Nathan already practices a version of this methodology.
  - Is the disclosure honest? Yes. Disclosure on each document specifies what AI contributed.

### Phase 2: Literature Review & Scholarly Positioning

- **Scholarly source handling**: AI organized sources; Nathan verified representations were accurate and fair.
- **Gap identification**: Nathan confirmed 4 persistent gaps match his reading of the field.
- **Counterarguments**: AI proposed strongest objections; Nathan evaluated whether these reflect actual peer review concerns.
- **Comparison fairness**: Nathan directed complementarity framing to avoid strawmanning competing frameworks.

### Phases 3-5: Implementation Guides

- **Faculty Guide**: Nathan's operational SCAD practice generalized to discipline-agnostic methodology. AI drafted; Nathan verified the translation preserved the substance while removing SCAD-specific dependencies.
- **Student Guide**: Written *to* students at Nathan's direction. The agency framing (students as developing epistemic agents, not potential cheaters) is Nathan's pedagogical position.
- **Institutional Guide**: Rollout model, mitigating factors clause, and assessment rubrics reflect Nathan's institutional experience. Accreditation crosswalk flagged as highest verification risk.

### Two-Level Architecture Integration (Post-Phase 5)

- **Architectural decision**: Nathan directed after reviewing all v1.0 deliverables. He recognized that the single-process model did not account for the fundamental difference between producing work and learning through work.
- **Propagation**: AI updated 6 documents for consistency. Nathan reviewed each for architectural coherence.
- **New scholarly basis**: 10 additional sources (from NotebookLM research) added to support human-first sequencing and Socratic gate design.

---

## The Ratio: By Decision Type

| Decision Type | Human | AI | Co-Created |
|--------------|-------|-----|-----------|
| **Strategic** (what to argue, what to include, who it serves) | 100% | 0% | 0% |
| **Architectural** (how to structure, what connects to what) | 60% | 10% | 30% |
| **Editorial** (voice, framing, audience register) | 70% | 20% | 10% |
| **Drafting** (turning decisions into prose) | 10% | 80% | 10% |
| **Verification** (checking accuracy, fairness, consistency) | 50% | 40% | 10% |
| **Tracking** (status updates, version management) | 5% | 90% | 5% |

**The pattern**: Human authority increases with epistemic weight. Strategic and architectural decisions are overwhelmingly human. Drafting and tracking are overwhelmingly AI. The middle ground (editorial, verification) is where the collaboration is most active and where the Five Questions do their work.

### Does This Pass ESF's Own Rigor Test?

The ratio above raises the question the framework itself demands: with AI handling 80% of drafting, does this work maintain genuine intellectual ownership?

Apply the Five Questions to the project as a whole:

1. **Can I defend this?** Yes. Nathan can explain every construct, every architectural decision, and every scholarly claim without referencing AI's reasoning. The constructs originated from his operational practice at SCAD. The two-level architecture was his judgment call. The counterarguments were evaluated against his knowledge of the field. If challenged in peer review, he can trace any claim back to his own position.

2. **Is this mine?** Yes, with a specific structure. The *arguments* are Nathan's. The *prose* is substantially AI-drafted within those arguments. This is the distinction ESF itself draws: intellectual ownership lives in the direction, emphasis, and judgment, not in the sentence-level drafting. The Directive Memo established what the framework should argue before AI produced any text. Every strategic and architectural decision in the table above was human. The 80% AI drafting ratio is high, but it operates within 100% human strategic authority.

3. **Did I verify?** Partially. 18 of 31 sources are confirmed or plan-verified. The 10 NotebookLM-sourced references and the accreditation crosswalk standard numbers carry verification risk that is documented and flagged, not concealed. This is an honest "not yet fully" rather than a false "yes." The framework's own protocol says: if the answer is no, fix it before proceeding. Publication-readiness requires completing this verification.

4. **Would I teach this?** Yes. Nathan already practices a version of this methodology in his courses and has used it to design curriculum across multiple disciplines. The framework formalizes and generalizes what he does. He can demonstrate it, not just describe it.

5. **Is the disclosure honest?** Yes. Every deliverable carries a disclosure statement specifying what AI contributed. This document makes the ratio explicit rather than vague. The limitations section acknowledges that the evidence is self-reported and that external validation is needed.

**Assessment: The project passes its own rigor test with one open item.** The strategic and architectural ownership is clear. The drafting delegation is high but operates within documented human authority. The verification gap (source #3) is acknowledged and flagged, not hidden. ESF does not require perfection at every gate; it requires honest assessment and a plan to address any "no." The plan here is pre-publication source verification.

---

## What This Demonstrates

1. **The Directive Memo works.** Having a documented pre-drafting position made it possible to evaluate every AI draft against a stable standard. Without it, drift detection would have been intuitive rather than systematic.

2. **Gates produce framework improvements.** The Phase 1 gate did not just verify the document; it improved the framework (adding the Gate Verification Record). Gates are generative, not just evaluative.

3. **Content Epistemic Weight calibration is practical.** Framework documents (high-weight) received full protocol. Tracking updates (low-weight) received light review. The graduated approach prevented protocol fatigue.

4. **The Five Questions catch real issues.** "Did I verify?" surfaced the accreditation crosswalk as highest verification risk. "Is this mine?" confirmed that the core argument and constructs originate from Nathan's practice, not from AI synthesis of the literature.

5. **Self-application reveals design flaws.** The two-level architecture emerged because applying ESF to its own student-facing deliverables exposed that a single production model could not serve both audiences. The framework was revised through its own use.

6. **AI excels at consistency; humans excel at judgment.** AI maintained cross-document terminology consistency across 15+ files. Nathan made every decision about what the framework argues, who it serves, and where it is vulnerable. The collaboration works because these are genuinely different capabilities.

---

## Design Rationale: Tool-Agnostic by Design

ESF was built with Claude Code (Anthropic) as the AI collaborator. Every document in the framework was produced through this specific human-AI partnership. Yet the framework itself never mentions Claude, never assumes any particular AI tool, and never recommends one. This is deliberate.

### The Evidence for Tool-Agnosticism

The scholarly basis for this design decision comes from multiple sources already in ESF's literature review:

**Metacognitive failures are tool-independent.** Tankelevitch et al. (2024) studied human oversight of AI output and found that the core challenge (detecting when AI output subtly departs from your own understanding) is a property of human cognition interacting with fluent AI output, not a property of any specific AI system. The Five Questions address a human vulnerability, not a tool-specific behavior.

**Cognitive automation generalizes across systems.** Atchley et al. (2024) demonstrated that repeated exposure to *any* AI-assisted workflow reduces the cognitive resources humans allocate to oversight. This means the Directive Memo, Human Validation Gates, and Gate Verification Records are needed regardless of tool choice. The framework's accountability mechanisms respond to how human cognition works, not how a specific AI works.

**The active endorsement condition is tool-neutral.** Clark and Chalmers (1998) specified that cognitive extension requires active endorsement of the tool's contributions. This condition applies identically to Claude, ChatGPT, Gemini, or any future system. The Directive Memo operationalizes active endorsement; the tool being endorsed is irrelevant to the mechanism.

**Educational technology evolves faster than adoption.** Zawacki-Richter et al. (2019) traced the evolution of AI in education through multiple generations. Each generation displaced prior tool-specific approaches. Luksha et al. (2024) argued that educational methodologies must build in adaptation mechanisms. ESF's Framework Evolution Protocol is designed for this reality: the framework evolves with the landscape rather than being locked to a snapshot of it.

### How This Played Out in Practice

The ESF development process itself demonstrates why tool-agnosticism matters:

- **The framework was built with Claude Code**, a specific tool with specific capabilities (long context, file editing, multi-agent orchestration). None of these capabilities are referenced in the framework documents.
- **The Directive Memo worked because of what Nathan wrote, not because of which AI read it.** The memo captured his intellectual position. Any capable AI system could have drafted from that position. The value was in the human's pre-drafting clarity, not in the tool's response.
- **The Five Questions applied to AI output regardless of its source.** When checking "Is this mine?" at each gate, Nathan evaluated his relationship to the content, not the tool's contribution to it.
- **NotebookLM (Google/Gemini) sourced the B.8 cluster.** The 10 additional scholarly sources in the Scaffolded Engagement cluster came from querying a different AI platform entirely. The framework's constructs applied identically to research discovered through a different tool.

### The Epistemic Stewardship Argument

The deeper rationale is philosophical: ESF's central claim is that intellectual ownership is maintained through the human's epistemic rigor, not through the tool's behavior. If the framework depended on specific tool features (a particular AI's tendency to hedge, or another's tendency to be confident), it would be a tool manual, not an epistemic methodology. The rigor must come from the human: from the Directive Memo that captures their position, from the Five Questions that test their engagement, from the gates that require documented evidence of genuine review. These mechanisms work because they structure *human* cognition, not because they respond to *AI* behavior.

This is why ESF prioritizes epistemic stewardship rigor over tool-specific guidance. Tools change. Models improve. Interfaces evolve. The human's responsibility to maintain intellectual ownership of their own work does not.

---

## Limitations of This Evidence

- **Self-reported.** The human-AI ratio is the author's assessment, not an independent audit.
- **Single practitioner.** This demonstrates ESF works for the person who designed it. External validation requires adoption by others.
- **AI collaboration logs are AI-produced.** The collaboration logs that document this process were themselves produced through AI collaboration, creating a recursive evidence challenge. The logs are accurate to the session records but are not independent verification.

---

> **AI Collaboration Disclosure:** This self-application evidence document was produced through human-AI collaboration following ESF protocol. Nathan established the document's purpose and directed what evidence to include. AI organized the evidence from collaboration logs, session records, and document history into the structure presented here. The human-AI ratio assessment (the table in "The Ratio" section) reflects Nathan's judgment about the division of labor across the project. AI assisted with organizing and presenting the evidence but did not determine the claims about intellectual ownership.

---

*Epistemic Stewardship Framework | Nathan Madrid | 2026*
*Self-Application Evidence Document*
