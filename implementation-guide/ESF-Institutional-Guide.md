---
title: "The Epistemic Stewardship Framework: Institutional Implementation Guide"
author: Nathan Madrid
date: 2026-02-18
version: "1.0-draft"
status: draft
type: implementation-guide
epistemic-weight: high
parent-document: ../framework/ESF-Framework-Document.md
tags: [esf, institutional-guide, implementation, policy]
---

# The Epistemic Stewardship Framework

## Institutional Implementation Guide

---

## A. The Case for Structured AI Integration

Higher education institutions face an AI integration challenge that no existing policy framework fully addresses. The challenge is not whether faculty and students will use AI. They already are. The question is whether the institution can maintain the intellectual integrity on which its credibility depends while enabling the productivity and pedagogical benefits AI offers.

Most institutions have responded with one of three approaches, each with predictable failure modes.

### The Ban

Some institutions or departments prohibit AI use outright. This approach is increasingly unenforceable as AI tools become embedded in standard software: browsers, word processors, email clients, search engines. More fundamentally, banning AI in an academic environment is pedagogically incoherent. Institutions that prepare students for professional practice cannot ignore the tools that define contemporary professional practice. The ban does not prevent AI use; it drives it underground, where it occurs without methodology, oversight, or disclosure.

### The Permissive Default

Other institutions permit AI use with minimal guidance, leaving individual faculty to set parameters. This creates inconsistency. Students encounter different rules in different courses, sometimes within the same program. Faculty who use AI in their own work have no shared methodology for maintaining intellectual ownership. The institution has no way to assess whether AI-assisted work meets its quality standards because those standards were developed before AI-assisted work existed.

### The Compliance Framework

The most common institutional response is a policy document: a set of rules about what AI use is permitted, what requires disclosure, and what constitutes a violation. Compliance frameworks are necessary but insufficient. They address the legibility of AI use (can we see what happened?) without addressing its quality (does the work reflect genuine intellectual engagement?). A student who transparently discloses AI use and submits work that reflects no original thinking has complied with the policy. The education has failed.

### What Is Needed

Institutions need a methodology, not just rules about AI use. They need a structured approach that:

- Provides faculty with a workflow for their own AI-assisted professional work
- Develops students' capacity for responsible AI collaboration through progressive engagement
- Gives institutional leaders tools for policy development, assessment, and accreditation
- Evolves as AI capabilities and institutional contexts change

The Epistemic Stewardship Framework (ESF) provides this methodology. It is tool-agnostic (not dependent on any specific AI product), discipline-agnostic (applicable across programs), and designed for phased adoption within existing institutional structures.

---

## B. Rollout Model

ESF adoption is designed to occur in three phases, each building on the previous. The model is intentionally conservative. It assumes that institutional culture changes slowly and that evidence of effectiveness must be demonstrated before expansion.

### Phase 1: Pilot (1-2 Terms)

**Goal:** Establish proof of concept with a voluntary faculty cohort.

**Scale:** 5-10 faculty across at least 3 disciplines. Diversity of department, rank, and AI experience is more important than size.

**Activities:**

- Faculty participants attend a half-day ESF orientation covering the five-phase workflow, Content Epistemic Weight, and the Directive Memo
- Each participant applies the full ESF workflow to at least one high-stakes deliverable per term (syllabus, assessment, scholarship, or accreditation material)
- Each participant uses the Course AI Policy template in at least one course
- Participants keep brief process logs (monthly, 15-minute reflections)
- A designated pilot coordinator collects process logs and schedules two cohort discussions per term

**Deliverables:**

- Pilot cohort establishes institutional baseline: What does faculty AI use currently look like? Where are the integrity risks?
- At least 5 completed Directive Memos and Integrity Reports (evidence of workflow application)
- Process log data identifying which ESF constructs are most and least useful in practice
- A pilot summary report with adoption recommendations

**Success criteria:**

- At least 70% of pilot participants report that the Directive Memo changed their relationship to AI output
- At least one participant identifies a case where the Five Questions caught an integrity issue that would have been missed without them
- No participant reports that ESF created unreasonable burden relative to its benefits
- Pilot summary report includes specific recommendations for Phase 2 expansion

**Common challenges:**

- *Self-selection bias.* Faculty who volunteer for an AI methodology pilot are likely already reflective about AI use. Phase 2 must address faculty who are less engaged.
- *Template overhead anxiety.* Some participants will initially perceive Directive Memos and Integrity Reports as bureaucratic burden. The orientation should emphasize that low-weight tasks require neither. The workflow scales with stakes.
- *Tool diversity.* Pilot participants will use different AI tools. This is a feature, not a bug. It tests ESF's tool-agnosticism.

---

### Phase 2: Expand (2-3 Terms)

**Goal:** Scale from individual faculty to department-level adoption and student-facing implementation.

**Scale:** 2-4 departments or programs, incorporating student engagement levels in selected courses.

**Activities:**

- Department-level workshops (2-3 hours) using pilot faculty as co-facilitators
- Each participating department designates an ESF coordinator (faculty member or chair)
- Faculty in participating departments apply the ESF workflow to at least one deliverable per term
- Selected courses implement student engagement levels (starting at Level 1-2; Levels 3-4 for advanced courses)
- Student Reflection templates piloted in participating courses
- Department coordinators collect adoption data quarterly

**Deliverables:**

- Department-level AI policies using the institutional AI policy template, adapted to disciplinary context
- Cross-department consistency report: Are ESF constructs interpreted consistently across disciplines?
- Student engagement data: Are students progressing through engagement levels? What are the barriers?
- Revised workshop materials incorporating pilot feedback

**Success criteria:**

- At least 50% of faculty in participating departments have applied the Directive Memo to at least one deliverable
- Student engagement level assessments show measurable progression (Level 1 to Level 2 within one term for introductory students)
- Department coordinators report that ESF integrates with existing processes rather than creating parallel structures
- At least one cross-disciplinary observation: a construct that works differently across departments, informing framework refinement

**Common challenges:**

- *Department culture variation.* STEM departments may find the Directive Memo more intuitive for lab reports and grants; humanities departments may find it more intuitive for syllabi and scholarship. Neither is wrong. The framework adapts.
- *Adjunct and contingent faculty.* Faculty with heavy teaching loads and limited institutional support need a lighter-touch version of the workflow. The "Getting Started" progression (from the Faculty Guide) is designed for this.
- *Student resistance.* Some students will perceive the Five Questions and Reflection templates as unnecessary overhead. Framing the methodology as professional skill development (not compliance) is essential.

---

### Phase 3: Institutionalize (Ongoing)

**Goal:** Embed ESF in institutional policy, faculty evaluation, student outcomes assessment, and accreditation.

**Scale:** Institution-wide, with disciplinary adaptation.

**Activities:**

- Institutional AI policy adopted using ESF as the methodological backbone
- ESF constructs referenced in faculty evaluation criteria (not as compliance checkboxes, but as evidence of professional practice)
- Student engagement levels mapped to program learning outcomes for assessment and accreditation
- Framework Evolution Protocol activated at institutional scale (annual review cycle, designated steward)
- New faculty orientation includes ESF overview
- Annual ESF review aligned with institutional assessment calendar

**Deliverables:**

- Institution-wide AI policy document referencing ESF methodology
- Updated faculty evaluation criteria incorporating ESF practices
- Program-level student AI engagement assessment mapped to accreditation standards
- Annual framework review report documenting what changed and why
- Accreditation materials demonstrating structured AI integration (see Accreditation Crosswalk)

**Success criteria:**

- ESF practices are normative: faculty use Directive Memos and disclosure without external prompting
- Student progression through engagement levels is tracked and reported at the program level
- Accreditation reviewers recognize the institution's AI integration as structured and evidence-based
- The Framework Evolution Protocol has produced at least one documented construct refinement or challenge

**Common challenges:**

- *Performative adoption.* The risk at institutional scale is that ESF becomes a compliance exercise, with faculty filling out templates without genuine engagement. The Gate Verification Record mechanism (documenting what was reviewed, changed, and challenged) is designed to counter this, but institutional culture is the primary defense.
- *Static policy.* Institutions tend to adopt policies and then leave them unchanged until a crisis. The Evolution Protocol's annual review cycle must be genuinely activated, not just documented.
- *Assessment burden.* Adding ESF metrics to existing assessment processes must not create unsustainable overhead. Integrate with existing assessment infrastructure rather than creating parallel reporting.

---

## C. Framework Stewardship and Evolution

ESF includes a built-in evolution mechanism: the Framework Evolution Protocol (Construct 7). At the institutional level, this protocol requires a designated steward and a structured review cycle.

### The Framework Steward

Every institution adopting ESF should designate a framework steward, an individual or committee responsible for:

- **Research intake:** Monitoring new scholarship on AI in education and assessing its implications for ESF constructs. On a quarterly or per-term basis, new sources are evaluated against the framework's theoretical grounding. Does new research reinforce, refine, or challenge existing constructs?
- **Challenge documentation:** When new evidence contradicts an ESF construct, documenting the contradiction transparently, not quietly absorbing the change through revision. The challenge record becomes part of the institutional framework documentation.
- **Revision management:** Proposing and implementing framework revisions through a versioned changelog. Each revision documents what changed, why, and what evidence supported the change.
- **Sunset assessment:** Periodically evaluating whether any construct should be retired or revised based on institutional evidence or external scholarship.
- **Cross-institutional learning:** Connecting with other ESF-adopting institutions (where applicable) to share implementation evidence and framework refinements.

### The Annual Review Cycle

The annual review aligns with the institutional assessment calendar and includes:

1. **Data collection** (beginning of cycle): Aggregate faculty adoption data, student engagement level progression, integrity case trends, and accreditation feedback.
2. **Literature scan** (mid-cycle): Review new scholarship for reinforcement, refinement, or challenge to existing constructs.
3. **Stakeholder feedback** (mid-cycle): Gather input from faculty, students, and department coordinators on framework usability and effectiveness.
4. **Review and revision** (end of cycle): Framework steward proposes any revisions, documents rationale, updates version number.
5. **Communication** (post-cycle): Share changes with the institutional community. Version notes explain what changed and why.

### Version Control

ESF at the institutional level maintains version numbers and a changelog:

- **Major versions** (1.0 to 2.0): Construct added, removed, or fundamentally redefined
- **Minor versions** (1.0 to 1.1): Construct refined, template updated, guidance clarified
- **Patch versions** (1.0.0 to 1.0.1): Typographic or formatting corrections only

The version history is part of the institutional documentation and is available for accreditation review.

---

## D. Policy Integration

ESF does not replace existing institutional policies. It provides the methodological infrastructure that makes those policies effective.

### Academic Integrity Policy

Most academic integrity policies define violations (plagiarism, fabrication, unauthorized collaboration) and consequences. AI challenges these policies because AI use is not inherently a violation. It depends on how it is used.

**ESF integration:** ESF provides the methodology that distinguishes responsible from irresponsible AI use. An academic integrity policy can reference ESF to clarify:
- AI use is permitted when it follows the ESF workflow (or an equivalent structured methodology)
- Disclosure is required per the ESF Disclosure Protocol
- The distinction between AI-assisted work (acceptable) and AI-substituted work (unacceptable) is assessed through the Five Questions and, where applicable, the Integrity Report
- Students at different engagement levels have different expectations (Level 1 students are learning; Level 4 students are expected to demonstrate full stewardship)

### Syllabus Requirements

Many institutions require a standard syllabus section on academic integrity. ESF provides a template (Course AI Policy) that faculty can customize for their discipline and course level.

**ESF integration:** The institutional policy can require that all syllabi include an AI use section based on the ESF Course AI Policy template, adapted to the course's discipline and level. This creates consistency without mandating identical policies across courses. The template provides structure while allowing faculty judgment.

### Faculty Evaluation

AI-assisted work by faculty raises questions about evaluation: how should tenure committees, peer reviewers, and department chairs assess work that involved AI collaboration?

**ESF integration:** The Directive Memo and Integrity Report provide evaluable evidence of the faculty member's intellectual contribution. Evaluation criteria can include:
- Evidence that the faculty member directed the intellectual content (Directive Memo)
- Evidence that the faculty member critically evaluated AI output (Integrity Report, Gate Verification Record)
- Transparent disclosure of AI's role in the work (Disclosure Protocol)
- Modeling of responsible AI practice for students (Course AI Policy)

This is not about penalizing AI use. It is about providing evidence that AI use was methodologically sound. Faculty who use AI well should be recognized for it, not despite it.

### Student Handbook

Student handbooks typically describe expectations, support resources, and conduct standards. ESF provides content for an AI use expectations section.

**ESF integration:** The handbook can describe:
- The institution's commitment to developing student AI collaboration capacity (not just regulating it)
- The four engagement levels as a developmental model students will encounter across their program
- The expectation of honest disclosure as a professional practice, not a policing mechanism
- Resources for developing AI collaboration skills (workshops, guides, faculty mentorship)

---

## E. Assessment of Framework Adoption

Assessing ESF adoption requires metrics at three levels: faculty, student, and institutional. These metrics are designed to integrate with existing assessment processes, not to create parallel reporting structures.

### Faculty Metrics

| Metric | Data Source | Frequency |
|--------|-----------|-----------|
| Adoption rate | Count of faculty applying ESF workflow | Per term |
| Directive Memo usage | Count of Directive Memos produced for high-weight content | Per term |
| Disclosure compliance | Percentage of AI-assisted deliverables with appropriate disclosure | Per term |
| Workshop participation | Attendance and follow-up application data | Per cycle |
| Self-reported impact | Faculty process log reflections | Annually |

### Student Metrics

| Metric | Data Source | Frequency |
|--------|-----------|-----------|
| Engagement level distribution | Course-level assessment of student placement across levels 1-4 | Per term |
| Level progression | Students advancing one or more levels within an academic year | Annually |
| Five Questions proficiency | Student Reflection template analysis (% answering "yes" to all five) | Per assignment |
| AI Use Log quality | Depth of kept/revised/rejected documentation and reflection | Per assignment |
| Disclosure specificity | Progression from generic ("I used AI") to specific disclosures | Longitudinal |

### Institutional Metrics

| Metric | Data Source | Frequency |
|--------|-----------|-----------|
| Cross-department consistency | Comparison of ESF construct interpretation across departments | Annually |
| Integrity case trends | AI-related integrity cases before and after ESF adoption | Annually |
| Accreditation alignment | ESF metrics mapped to accreditor standards (see Crosswalk) | Per cycle |
| Evolution Protocol activation | Documented challenges, revisions, and version changes | Annually |
| Student outcome correlation | Relationship between engagement level and program outcomes | Longitudinal |

### Assessment Principles

- **Measure practice, not compliance.** The goal is to assess whether ESF is developing capacity (faculty methodology, student agency), not whether boxes are checked.
- **Use existing infrastructure.** Integrate ESF metrics with existing program assessment, course evaluation, and institutional reporting. Avoid creating a separate ESF assessment layer.
- **Track trends, not thresholds.** Improvement over time (increasing adoption, deeper reflections, higher engagement levels) matters more than hitting specific benchmarks.
- **Report honestly.** If ESF adoption is slow, student progression is uneven, or certain constructs are underused, document this. The framework's Evolution Protocol depends on honest data about what works and what does not.

---

## F. Getting Started for Institutional Leaders

ESF adoption does not require institutional mandate from the outset. Most successful adoptions begin informally.

1. **Identify a champion.** Find a faculty member, department chair, or center for teaching and learning director who is already thinking about AI integration methodology. This person becomes the pilot coordinator.

2. **Start with faculty practice.** The most compelling case for ESF is made when institutional leaders see faculty applying it to their own work, not when they read about it in a policy document. Begin with the Faculty Guide, not the Institutional Guide.

3. **Run a pilot.** Five faculty members, one term, one high-stakes deliverable each. Collect evidence. The pilot summary report is your case for expansion.

4. **Connect to existing priorities.** ESF maps to institutional priorities that likely already have attention and resources: accreditation readiness, faculty development, student outcomes assessment, academic integrity, and institutional reputation. Frame ESF as a tool for existing priorities, not a new initiative requiring new resources.

5. **Plan for evolution.** The institution that adopts ESF in 2026 will need a different version in 2028. Build the Evolution Protocol into your adoption plan from the beginning. Designate a steward, schedule the annual review, commit to version transparency. A framework that does not evolve will be replaced.

---

> **AI Collaboration Disclosure:** This institutional guide was developed through human-AI collaboration. The rollout model, policy integration framework, and assessment metrics are derived from the ESF Framework Document and informed by the author's experience with institutional AI integration at an art and design university. AI assisted with structural organization, prose drafting, and the construction of cross-institutional scenarios. All policy recommendations, assessment frameworks, and claims about institutional practice reflect the author's professional judgment. This document follows the ESF protocol it describes.

---

*Version 1.0-draft | 2026-02-18*
*Epistemic Stewardship Framework â€” Institutional Implementation Guide*
*Nathan Madrid*
