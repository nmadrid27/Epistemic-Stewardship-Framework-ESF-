---
title: "ESF Assessment Rubrics"
author: Nathan Madrid
date: 2026-02-18
version: "1.0-draft"
status: draft
type: assessment-rubrics
epistemic-weight: medium
parent-document: ../framework/ESF-Framework-Document.md
tags: [esf, assessment, rubrics, institutional]
---

# ESF Assessment Rubrics

## Evaluating Framework Adoption at Faculty, Student, and Institutional Levels

---

## Purpose

These rubrics provide structured criteria for assessing ESF adoption quality. They are designed for use by department chairs, assessment directors, faculty development coordinators, and institutional leaders. The rubrics assess the depth and quality of ESF practice, not compliance with rules but evidence of genuine epistemic engagement.

---

## Rubric 1: Faculty Implementation Quality

*Use for: Faculty evaluation, faculty development assessment, peer review of AI-assisted professional work.*

| Dimension | Developing | Proficient | Advanced |
|-----------|-----------|-----------|----------|
| **Directive Memo Practice** | Writes memos for some high-stakes work; memos are generic or lack specificity in thesis, emphasis, or non-negotiables | Writes memos consistently for high and medium-weight work; memos are specific and actionable; Ownership Test completed | Memos demonstrate deep disciplinary expertise; position is clearly articulated and defensible; memos improve over time with visible refinement |
| **Content Epistemic Weight** | Can distinguish high from low weight for obvious cases; occasionally misclassifies medium-weight content | Classifies weight accurately across content types; recognizes context-dependent weight shifts (e.g., syllabus for new course = high) | Weight classification is intuitive and nuanced; can articulate the reasoning for edge cases; helps colleagues calibrate their own classifications |
| **Five Questions Application** | Applies Five Questions inconsistently; some questions answered reflexively rather than reflectively | Applies Five Questions at every gate; answers reflect genuine evaluation; can identify which questions are hardest for different content types | Five Questions practice is habitual and deep; has identified personal patterns (e.g., "I am most likely to drift on Question 2"); uses insights to improve workflow |
| **Disclosure and Integrity Reports** | Includes disclosure on most AI-assisted work; disclosure is sometimes generic | Disclosure is specific to each deliverable; Integrity Reports produced for high-weight content; AI contribution levels assessed accurately | Disclosure is exemplary and could serve as a model for colleagues; Integrity Reports include Gate Verification Records with substantive documentation of what was reviewed, changed, and challenged |
| **Student-Facing Practice** | Includes an AI policy in syllabus; discusses AI use with students | Course AI Policy is detailed and discipline-appropriate; models own AI practice for students; assesses student engagement levels | AI collaboration pedagogy is integrated into course design; student progression through engagement levels is tracked and supported; contributes to institutional AI methodology |
| **Framework Evolution** | Uses ESF as received; does not contribute feedback | Identifies areas where ESF works well or poorly in their discipline; shares feedback through institutional channels | Proposes construct refinements or discipline-specific adaptations; contributes to the institution's annual framework review |

---

## Rubric 2: Student Engagement Progression

*Use for: Course-level assessment, program-level outcome evaluation, accreditation evidence.*

| Dimension | Level 1: Discovery | Level 2: Guided Use | Level 3: Independent Use | Level 4: Critical Partnership |
|-----------|-------------------|--------------------|--------------------------|---------------------------------|
| **AI Understanding** | Explores AI capabilities; recognizes that AI output requires evaluation | Understands AI strengths and weaknesses for their discipline; can articulate limitations | Assesses AI appropriateness for specific tasks; selects tools and approaches deliberately | Evaluates AI capabilities critically; understands underlying mechanisms; can assess new tools independently |
| **Position Statement** | Not yet practiced | Writes brief Position Statements with instructor guidance; captures basic position | Writes complete Position Statements independently; position, emphasis, and non-negotiables are specific and grounded | Position Statements demonstrate sophisticated disciplinary thinking; can articulate why specific non-negotiables matter |
| **Five Questions** | Becoming aware of the need for critical evaluation | Applies Five Questions with instructor prompting; most answers are "yes" | Self-applies Five Questions without prompting; identifies and addresses "no" answers | Five Questions practice is habitual; can articulate personal evaluation patterns; mentors peers |
| **Editorial Judgment** | Accepts or rejects AI output in bulk | Keeps, revises, or rejects AI output with basic reasoning | Kept/revised/rejected decisions demonstrate nuanced evaluation; revision rationale is specific | Editorial judgment is sophisticated; can explain why specific framing choices, structural decisions, or content elements were revised or rejected |
| **Disclosure Practice** | Learning that disclosure is expected | Discloses AI use with basic descriptions | Disclosure is specific, honest, and differentiated by contribution type | Disclosure demonstrates professional-level transparency; could serve as a model for peers |
| **Verification** | Relies on AI accuracy; beginning to check | Verifies factual claims and citations when prompted | Independently verifies; has developed systematic verification practices | Verification is automatic and thorough; can identify subtle accuracy issues; teaches verification strategies |
| **Metacognitive Awareness** | Limited awareness of own process | Recognizes when AI output has influenced their thinking | Monitors for cognitive drift; adjusts approach when detecting over-reliance | Articulates personal AI collaboration methodology; identifies strengths and vulnerabilities in own practice |

### Scoring Guidance

- Students are assessed on their **highest consistent level**, not their peak performance
- Students may demonstrate different levels across different dimensions. This is normal and informative.
- Progression is **expected over time**, not within a single assignment
- Level 4 is an aspirational standard for advanced students, not a graduation requirement
- Students who honestly assess themselves at Level 1-2 demonstrate the metacognitive awareness that enables progression

---

## Rubric 3: Institutional Adoption Maturity

*Use for: Institutional self-assessment, accreditation self-study, strategic planning.*

| Dimension | Emerging | Developing | Established | Leading |
|-----------|---------|-----------|-------------|---------|
| **Policy Infrastructure** | AI policy exists but lacks methodological specificity; individual faculty set own parameters | AI policy references a structured methodology (ESF or equivalent); course AI policies expected in syllabi | Institution-wide AI policy with methodological backbone; all syllabi include AI use sections; policy reviewed annually | AI methodology is embedded in institutional culture; policy evolution is data-driven; institution contributes to cross-institutional learning |
| **Faculty Adoption** | Individual faculty experimenting with structured AI use; no shared methodology | Pilot cohort using ESF; workshops available; adoption data collected | Majority of faculty in participating departments using ESF; department-level coordination in place | ESF practices are normative; faculty use methodology without external prompting; new faculty orientation includes AI methodology |
| **Student Development** | AI use discussed but not systematically developed; no engagement level framework | Engagement levels introduced in selected courses; Student Reflection templates in use | Engagement level progression tracked at program level; student AI capacity is a program learning outcome | Student progression data informs curriculum design; students demonstrate Level 3-4 capacity at program completion; graduates enter professions with practiced AI methodology |
| **Assessment Integration** | AI-related assessment is ad hoc or absent | ESF metrics collected in pilot courses; baseline data established | ESF metrics integrated with existing program assessment; trends tracked longitudinally | AI collaboration capacity is assessed as a core competency; data drives curriculum revision; accreditation materials showcase structured AI integration |
| **Framework Evolution** | No systematic review of AI policies or practices | Annual review planned; steward designated | Annual review cycle active; at least one documented revision based on evidence | Evolution Protocol fully operational; challenges documented; version history demonstrates data-driven refinement; institution contributes to scholarship on AI integration |
| **Accreditation Readiness** | AI use mentioned in accreditation materials but not systematically documented | ESF crosswalk consulted; some evidence collected for accreditation standards | Accreditation materials include ESF evidence mapped to specific standards; reviewers recognize structured approach | AI integration methodology is a strength in accreditation review; evidence is comprehensive and longitudinal; institution serves as a model for peers |

### Maturity Assessment Guidance

- Most institutions beginning ESF adoption will be at **Emerging** or **Developing**
- **Established** is a realistic target after 2-3 years of intentional adoption
- **Leading** represents aspirational practice; few institutions will achieve this across all dimensions in the near term
- Use the assessment for **strategic planning**, not evaluation. Identify where investment will have the most impact.
- Honest assessment at a lower maturity level is more valuable than inflated assessment at a higher one

---

## Using These Rubrics

### For Faculty Evaluation
Use Rubric 1 as one component of faculty evaluation, not as a checklist but as a framework for discussion. The rubric identifies dimensions of practice; the conversation identifies growth opportunities.

### For Student Assessment
Use Rubric 2 within courses to track student development. Aggregate across courses and terms for program-level assessment. The rubric is formative: it describes a developmental trajectory, not a minimum standard.

### For Institutional Planning
Use Rubric 3 annually for institutional self-assessment. Compare across dimensions to identify uneven adoption. Share results with the framework steward to inform the annual review cycle.

### For Accreditation
All three rubrics provide evidence for accreditation standards related to academic integrity, student learning outcomes, faculty engagement, and continuous improvement. See the ESF Accreditation Crosswalk for specific standard-to-evidence mapping.

---

> **AI Collaboration Disclosure:** These rubrics were developed through human-AI collaboration. The rubric dimensions are derived from ESF constructs; the performance level descriptions synthesize the framework's theoretical grounding with the author's assessment design experience. AI assisted with tabular organization, parallel structure across performance levels, and consistent formatting. All evaluative criteria and progression expectations reflect the author's professional judgment about what constitutes meaningful AI collaboration practice.

---

*Version 1.0-draft | 2026-02-18*
*Epistemic Stewardship Framework: Assessment Rubrics*
*Nathan Madrid*
