---
title: "ESF Social Media Posts"
type: social-content
last-updated: 2026-02-24
---

# ESF Social Media Content

LinkedIn posts, threads, and short-form writing about the Epistemic Stewardship Framework.

## Purpose

Build visibility for the framework before and during the submission/publication process. Establish Nathan's voice on epistemic stewardship, AI-assisted academic work, and faculty responsibility in AI integration.

## Post Index

| # | File | Platform | Format | Topic | Status |
|---|------|----------|--------|-------|--------|
| 01 | `01-what-is-esf.md` | LinkedIn | Long-form | Framework introduction | DRAFT |
| 02 | `02-two-levels.md` | LinkedIn | Long-form | Two-Level Architecture | DRAFT |
| 03 | `03-directive-memo.md` | LinkedIn | Short | Directive Memo | DRAFT |
| 04 | `04-five-questions.md` | LinkedIn | Short | Five Questions | DRAFT |
| 05 | `05-records-of-resistance.md` | LinkedIn | Long-form | Records of Resistance | DRAFT |
| 06 | `06-in-practice.md` | LinkedIn | Long-form | Framework in the classroom | DRAFT |
| 07 | `07-epistemic-weight.md` | LinkedIn | Short | Epistemic weight / calibration | DRAFT |
| 08 | `08-bootstrapping.md` | LinkedIn | Long-form | Bootstrapping tension | DRAFT |

## Conventions

- **File naming:** `##-topic-slug.md` (numbered for sequence)
- **Frontmatter:** title, platform, topic, status (draft/ready/posted), posted-date, word-count
- **Length targets:** LinkedIn long-form = 700-1,200 words; LinkedIn short = 150-300 words
- **Voice:** Direct, substantive, practitioner-facing. Not academic. No jargon without immediate plain-language translation.
- **Framework language in public posts:** Lead with the problem (losing yourself in the output), not the solution name. Let readers arrive at the framework; don't announce it.
- **Institution-neutral:** No course codes, department names, or institutional identifiers. Use generic descriptors ("two courses," "introductory and intermediate AI courses").
- **Cadence:** Weekly posting. 8-week arc timed to publication prep.
- **Repo promotion:** GitHub repo (`github.com/nmadrid27/Epistemic-Stewardship-Framework-ESF-`) is referenced starting post 03. Never put the link in the post body (hurts algorithm reach). Reference the resource in the post; drop the URL in the first comment.
- **Repo mention cadence:** Posts 01-02 = none. Posts 03-05 = light ("open-sourced, link in comments"). Posts 06-08 = stronger ("full framework and toolkits are open source").

## Content Sequence

### Phase 1: Establish the Problem (Posts 01-02)

1. **The core problem.** AI generates fluent content that doesn't belong to you unless you actively work to make it yours. *Hook: "We're getting better at producing content and less sure it's actually ours."*
2. **The Two-Level Architecture.** Faculty managing production is not the same as students developing epistemic habits. Same tools, fundamentally different problems. *Hook: "Why do AI policies that work for faculty fail for students?"*

### Phase 2: Share the Tools (Posts 03-05)

3. **Directive Memo.** What it means to direct AI rather than follow it. The document you write before opening any AI tool. *Hook: "Who's directing the work: you or the AI?"*
4. **The Five Questions.** A portable accountability checkpoint anyone can apply in 60 seconds. *Hook: "A 60-second test for whether the work is yours."*
5. **Records of Resistance.** Why keeping what you rejected matters as much as what you kept. The most counterintuitive construct in the framework. *Hook: "What you rejected teaches you as much as what you kept."*

### Phase 3: Evidence and Nuance (Posts 06-07)

6. **Framework in practice.** What actually changed when this was tested in introductory and intermediate AI courses. Classroom evidence, not theory. *Hook: "What happened when I actually tested this in the classroom."*
7. **Epistemic weight.** Not all AI-assisted content is the same risk. A calibration tool for deciding when methodology matters most. *Hook: "Not all AI-assisted work carries the same risk."*

### Phase 4: Meta-Reflection (Post 08)

8. **The bootstrapping tension.** Can you use AI to build a framework about responsible AI use? Yes, with documentation. The most intellectually honest post in the series. *Hook: "Can you use AI to build a framework about responsible AI use?"*
