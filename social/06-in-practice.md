---
title: "What Actually Happened"
platform: LinkedIn
topic: Framework in the classroom
status: draft
posted-date:
word-count: ~300 (short) / ~450 (mid) / ~550 (long)
---

# What Actually Happened (DRAFT)

---

**Post Option A: Short-Form (Recommended for LinkedIn)**

I tested the epistemic stewardship framework in two courses this past term. Here's what actually happened, not the theory.

The hardest part wasn't getting students to use AI responsibly. It was getting them to write down what they thought before they opened the tool.

That step, forming a position before AI enters the conversation, met real resistance. Not because students didn't understand it. Because it's uncomfortable to commit to a position when you know AI can give you a better-sounding one in seconds.

The students who pushed through that discomfort reported something I didn't expect: they started having a clearer sense of when the AI was helping and when it was replacing them. The position statement gave them something to compare against. Without it, they said they couldn't tell the difference.

The other surprise was the records of resistance. When I asked students to log what they rejected from AI output and why, several said the "rejected" entries were more useful than the "kept" entries. Saying no to AI forced them to articulate what they actually believed.

I don't want to oversell this. It's two courses at one institution, assessed by the person who designed the framework.

That's practitioner evidence, not a controlled study. But it's operational evidence that the constructs work in practice, not just in theory.

What I'd tell anyone considering something like this: start with your own work first. I used the framework for a full term on my own professional work before I asked students to use it. That mattered.

The full framework, toolkits, and implementation guides are open source. Link in the first comment.

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #HigherEducation #FacultyDevelopment #FutureOfWork*

---

**Post Option B: Mid-Form**

I've been sharing ideas about epistemic stewardship for the past few weeks. Here's what it actually looked like when I tested it.

I integrated the framework into two courses this past term: one introductory-level, focused on how AI intersects with creative practice, and one intermediate, focused on building with AI tools. Different student populations, different objectives, same underlying methodology.

The hardest part wasn't getting students to use AI responsibly. It was getting them to write down what they thought before they opened the tool. Students resisted it at first. Writing a position when you know AI can generate a polished one in seconds feels pointless. Why commit to something rough when something smooth is immediately available?

But the students who did it consistently reported a shift I didn't anticipate: they developed a clearer sense of when AI was helping them and when it was replacing them. The position statement gave them a reference point. Without it, they said, they couldn't tell the difference between "I agree with this" and "I can't remember what I was going to say."

The other surprise was the records of resistance. Multiple students said the "rejected" column was more useful than the "kept" column. Saying no to AI output, and writing down why, forced them to articulate positions they didn't know they held.

The overhead was real. Students initially perceived the position statement and the logging as busywork. It took several weeks before most of them stopped seeing it as a compliance exercise and started seeing it as useful. I'm not sure everyone got there.

I don't want to oversell this. It's two courses at one institution, assessed by the person who designed the framework. That's practitioner evidence, not a controlled study. But it's operational evidence. These constructs work in practice, not just in theory.

What I'd tell anyone considering something like this: start with your own work first. I used the framework for a full term on my own professional work before I asked students to use it. When a student asked "do you actually do this?" I could show them my directive memos and integrity reports. That mattered more than I expected.

The full framework, toolkits, and implementation guides are open source. Link in the first comment.

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #HigherEducation #FacultyDevelopment #FutureOfWork*

---

**Post Option C: Long-Form**

I've been sharing ideas about epistemic stewardship for the past few weeks. Here's what it actually looked like when I tested it.

I integrated the framework into two courses this past term: one introductory-level, focused on how AI intersects with creative practice, and one intermediate, focused on building with AI tools. Different student populations, different objectives, same underlying methodology.

---

**What worked.**

The position statement (writing down what you think before AI enters the process) was the single most impactful intervention. Students resisted it at first. Writing a position when you know AI can generate a polished one in seconds feels pointless. Why commit to something rough when something smooth is immediately available?

But the students who did it consistently reported a shift I didn't anticipate: they developed a clearer sense of when AI was helping them and when it was replacing them. The position statement gave them a reference point. Without it, they said, they couldn't tell the difference between "I agree with this" and "I can't remember what I was going to say."

That tracks with what I experience in my own work.

The other surprise was the records of resistance, the log of what students kept, revised, and rejected from AI output. Multiple students said the "rejected" column was more useful than the "kept" column. Saying no to AI output, and writing down why, forced them to articulate positions they didn't know they held.

---

**What was hard.**

The overhead. Students initially perceived the position statement and the logging as busywork.

It took several weeks before most of them stopped seeing it as a compliance exercise and started seeing it as useful. I'm not sure everyone got there.

The self-assessment honesty. When students apply the Five Questions ("Is this mine?" "Can I defend this?"), they're self-reporting.

Some were clearly more honest than others. The framework depends on honest engagement, and that can't be enforced from outside.

My own credibility test. I used the full framework on my own professional work for a term before I asked students to use it. That mattered more than I expected.

When a student asked "do you actually do this?" I could show them my directive memos and integrity reports. If I hadn't, I don't think the framework would have had the same weight.

---

**What I'd say honestly.**

This is two courses at one institution, assessed by the person who designed the framework. That's practitioner evidence, not a controlled study. I'm transparent about the limitations because the framework demands it.

But it's also operational evidence. These constructs work in practice, not just in theory.

Students used them. Their work changed. Their self-reported relationship to AI output changed. That's worth something, even at small scale.

The full framework, implementation guides, and toolkits are open source. Link in the first comment.

If you've tried anything similar, structured approaches to how students or teams engage with AI, I'd like to hear what you found. This is early, and the more perspectives, the better.

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FacultyDevelopment #FutureOfWork #TeachingWithAI*
