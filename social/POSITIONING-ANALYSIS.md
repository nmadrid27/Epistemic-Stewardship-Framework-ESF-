---
title: "ESF Social Content: Positioning Analysis"
date: 2026-02-27
type: strategy-document
status: reference
---

# ESF Social Content: Positioning Analysis

An honest assessment of whether and how to share the Epistemic Stewardship Framework publicly, based on competitive landscape research conducted February 2026.

---

## The Competitive Landscape

### Existing Frameworks in AI + Higher Education

| Framework | Source | What It Does | What It Doesn't Do |
|-----------|--------|-------------|-------------------|
| **AIAS** (Perkins et al., 2024) | Peer-reviewed, globally adopted (hundreds of institutions, 30+ languages) | Assessment scale: tells instructors how much AI to allow in student work (5 levels, "no AI" to "full AI") | Does not address faculty use. No pre-drafting mechanism. No intellectual ownership methodology. |
| **ETHICAL Principles** (CSU, 2025) | Cal State system, AAC&U Institute | Values-oriented principles for responsible AI use across academic contexts | Flexible but not operational. No workflow. No production methodology. |
| **WCET Ecosystem Framework** (2025) | WICHE Cooperative for Educational Technologies | Strategic, action-oriented policy roadmap for institutions | Policy-level. Not practitioner-facing. No methodology for individual use. |
| **AAUP Report** (May 2025) | Ad hoc Committee on AI and Academic Professions | Addresses faculty concerns about academic freedom, IP, and evaluation | Policy document. No structured methodology for faculty AI use. |
| **HEAT-AI** (Frontiers, 2025) | Peer-reviewed | Regulatory framework for institutional AI governance | Regulatory, not practitioner-level. |
| **UNESCO/OECD Guidance** (2023) | International policy bodies | Transparency, human agency, ethical principles | Compliance-oriented. No operational methodology. |

### The Gap ESF Fills

None of the existing frameworks provide:

1. A **faculty-facing production methodology** (workflow for using AI in professional academic work)
2. A **pre-drafting intellectual authority mechanism** (writing your position before AI is engaged)
3. A **structural distinction** between production and development (two-level architecture)
4. **Practitioner-tested implementation** with open-source toolkits
5. **Self-application evidence** (framework built using its own methodology, documented)

The dominant question in the field is "how much AI should we allow?" ESF asks a different question: "how do you maintain ownership of your thinking?" That reframe is genuine and unfilled.

### Emerging Research in the Same Space (2025-2026)

Papers appearing at Frontiers in Education, Taylor & Francis, and MDPI are increasingly addressing epistemic agency, epistemic authority, and the cognitive effects of AI on knowledge production. Key themes:

- **Epistemic authority and AI** (Frontiers, 2025): How AI challenges who holds knowledge authority in classrooms
- **AI and epistemic agency** (Taylor & Francis, 2025): How AI influences belief formation and revision
- **Epistemic responsibility in human-AI collaboration** (PMC, 2025): Toward community standards

The conceptual territory is getting attention. The operational territory (structured methodology for practitioners) remains open.

---

## ESF's Distinctive Strengths

### 1. The Directive Memo (Genuinely Novel)

No published framework requires a human-only pre-drafting phase that establishes intellectual authority before AI engagement. AIAS defines AI involvement levels but doesn't structure how human direction is established. The Directive Memo fills a structural gap that the literature has identified (Clark & Chalmers's active endorsement condition, Wu et al.'s epistemic agency concerns) but nobody else has operationalized.

### 2. Two-Level Architecture

Nobody else makes the structural distinction between production (faculty, practitioners) and development (students, learners) with different process models and different checkpoints serving different purposes. Existing frameworks address students OR practitioners. ESF addresses both and connects them: faculty who practice Level 1 create the environment for Level 2 development.

### 3. Practitioner-First Origin

Most frameworks are theorized then tested. ESF was built from months of operational practice (curriculum development, assessment design, institutional reporting, course delivery), then the literature was found to support it. This is the opposite direction from most published frameworks, and it's more credible for a practitioner audience.

### 4. Open Source

GitHub repo with full framework documentation, installable student and faculty toolkits (curl install), CC BY 4.0 license. Most frameworks are locked behind publications, institutional walls, or consulting fees. Open-sourcing signals generosity, invites scrutiny, and builds trust.

### 5. Self-Application Evidence

The framework was built using AI collaboration and documents its own process: directive memos, contribution ratios, gate verification records, and honest documentation of edge cases (the bootstrapping memo). "Here's the work. Judge it." is the strongest possible positioning for credibility.

---

## Honest Vulnerabilities

### Evidence Base

- Single practitioner, single institution. Two courses. Self-reported.
- Practitioner evidence in the reflective tradition, not a controlled study.
- The framework's own documentation is transparent about this limitation.

### Publication Status

- Not yet peer-reviewed. In academic circles, unpublished = unvalidated.
- LinkedIn builds visibility; publication builds academic credibility. Both are needed.
- The social series should complement the publication path, not replace it.

### "Framework" Fatigue

- Everyone on LinkedIn has a framework. The word triggers skepticism.
- The social posts lead with problems, not the framework label. This is correct.
- The term "epistemic stewardship" is an existing concept Nathan is applying, not a coined term. This is honest and avoids the "branded framework" trap.

### Audience Size

- Starting from a modest LinkedIn following means slow organic growth.
- 50-100 impressions per post is normal early on.
- Content compounds over time. Don't judge the series by post 01's performance.
- The first 60-90 minutes of engagement (responding to comments) determine algorithmic reach.

---

## Predictable Pushbacks and Responses

**"This is just good practice. Why does it need a framework?"**
Because good practice drifts without structure, and structure without a name can't be shared, taught, or studied. Intuitive calibration works until you're tired, busy, or deadline-pressed. Then medium-weight work gets treated as low-weight. That's the drift the framework makes visible.

**"This is too much overhead."**
Content Epistemic Weight exists for exactly this reason. A meeting agenda doesn't need a directive memo. A strategic recommendation does. The framework scales with stakes. Low-weight content gets light review and attribution. The full methodology is reserved for high-weight work.

**"You haven't published this."**
True. The framework is in pre-publication preparation. The social series builds visibility for the concepts while the peer review process runs. The open-source repo means anyone can evaluate the work now, not after publication.

**"You built this with AI. Isn't that a contradiction?"**
No. The framework is designed for AI-assisted work, not against it. The self-application evidence documents the entire process: directive memos, contribution ratios, edge cases. The bootstrapping tension (post 08) addresses this directly.

---

## Strategic Recommendation

**Publish the social series. The risk of waiting is higher than the risk of sharing.**

- The gap ESF fills is real and documented.
- The conceptual territory is attracting academic attention; the operational territory is still open.
- Open source differentiates from every other framework in the space.
- The cross-sector framing (not just education) expands the audience ceiling.
- Honest positioning (practitioner working through this, not authority announcing a solution) is the right tone.

The risk of sharing is that it lands quietly. The risk of not sharing is that someone else fills the gap without the depth of work that's been done.

---

## Sources Consulted

- [AIAS Framework](https://aiassessmentscale.com/) (Perkins et al., 2024; revised 2024)
- [AIAS Revisited](https://arxiv.org/abs/2412.09029)
- [ETHICAL Principles AI Framework, CSU](https://genai.calstate.edu/communities/faculty/ethical-and-responsible-use-ai/ethical-principles-ai-framework-higher-education)
- [AAUP: AI and Academic Professions](https://www.aaup.org/reports-publications/aaup-policies-reports/topical-reports/artificial-intelligence-and-academic-professions)
- [Epistemic Authority and Generative AI (Frontiers, 2025)](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1647687/full)
- [AI Education Policy Ecosystem Framework, WCET 2025](https://learnworkecosystemlibrary.com/initiatives/ai-education-policy-guidelines-practice-ecosystem-framework-2025-wcet/)
- [AI in Higher Education: Pedagogical Integrity (MDPI)](https://www.mdpi.com/2673-8392/5/4/180)
- [Navigating AI in Higher Ed 2025-2026, U. Sydney](https://educational-innovation.sydney.edu.au/teaching@sydney/navigating-ai-in-higher-education-tasks-ahead-for-2025-and-2026/)
- [AI and Epistemic Agency (Taylor & Francis, 2025)](https://www.tandfonline.com/doi/full/10.1080/02691728.2025.2466164)
- [Epistemic Responsibility in Human-AI Collaboration (PMC, 2025)](https://pmc.ncbi.nlm.nih.gov/articles/PMC12271224/)
- [HEAT-AI Framework (Frontiers, 2025)](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2025.1505370/full)
- [LinkedIn engagement data: ConnectSafely 2026](https://connectsafely.ai/articles/linkedin-post-examples-high-engagement-2026)
- [LinkedIn thought leadership best practices: Rosica 2026](https://www.rosica.com/2026/02/23/linkedin-thought-leadership-tips-for-2026)

---

*Analysis conducted 2026-02-27. Update as landscape shifts.*
