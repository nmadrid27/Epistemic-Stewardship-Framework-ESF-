---
title: "What Is Epistemic Stewardship?"
platform: LinkedIn
topic: Framework introduction
status: draft
posted-date:
word-count: ~300 (short) / ~400 (mid) / ~550 (long)
---

# What Is Epistemic Stewardship? (DRAFT)

---

**Post Option A: Short-Form (Recommended for LinkedIn)**

We're producing more with AI and understanding less of what we produce.

It's something I notice in my own work. I'll approve an AI-generated draft, and it reads well, and I realize I'm not sure which ideas are mine and which ones I'm just agreeing with because they sound right.

The problem isn't AI. The problem is that fluent output feels like understanding, even when you didn't do the thinking that produced it. Researchers studying AI-assisted work have a term for this: the illusion of fluency. Polished output triggers a sense of comprehension, even when the reader didn't produce the reasoning behind it.

I've been working through this for the past year, in my teaching and in my own practice. Researching it. Testing approaches in courses.

Trying to figure out what it actually takes to stay the intellectual author of work you produce with AI, not just the person who approved it.

The concept I keep coming back to is **epistemic stewardship**: owning your thinking, not just your output. It's not a new idea. But applying it when you're working with AI is where it gets hard.

It's bigger than education. Anyone producing professional work with AI faces it: consultants, managers, writers, researchers, creatives.

The question is the same everywhere. At what point does the output stop being yours?

I don't have a clean answer. But I've found that the question itself changes how you work, if you take it seriously.

I'll be sharing what I've found in upcoming posts. But I'm curious: if you use AI in your work, how do you know when the output is still yours? Is there a moment where you feel the balance shift?

---

*#AIinEducation #EpistemicStewardship #FutureOfWork #AILiteracy*

---

**Post Option B: Mid-Form**

We're producing more with AI and understanding less of what we produce.

It's something I notice in my own work. I'll approve an AI-generated draft, and it reads well, and I realize I'm not sure which ideas are mine and which ones I'm just agreeing with because they sound right.

AI tools are useful. They compress time. They clear blocks. They surface connections I wouldn't have made alone.

But there's a version of this where the tool does the thinking and I do the approving. If I'm honest, that version is seductive. It moves fast. It looks right.

The problem isn't AI. The problem is that fluency is not the same as understanding. Researchers studying AI-assisted work have a term for this: the illusion of fluency. Polished output triggers a sense of comprehension, even when the reader didn't produce the reasoning behind it. Fluent output that I didn't build doesn't represent my judgment, even when I agree with it.

This isn't unique to education. A consultant reviewing an AI-drafted deliverable faces it. A manager approving a report their team produced with AI faces it. A writer publishing AI-assisted content faces it.

The question is the same everywhere: at what point does the output stop being yours?

I've been working through this for the past year, in my teaching and in my own practice. Researching it. Testing approaches in courses. The concept I keep coming back to is **epistemic stewardship**: owning the thinking behind the work, not just the output. It's not a new term, but it names the gap I keep encountering: the difference between using AI to build on your ideas and using AI instead of having ideas.

I don't have a clean answer. But I've found that the question itself changes how you work, if you take it seriously.

I'll be sharing what I've found in upcoming posts. But I'm curious: if you use AI in your work, how do you know when the output is still yours?

---

*#AIinEducation #EpistemicStewardship #FutureOfWork #AILiteracy*

---

**Post Option C: Long-Form**

We're producing more with AI and understanding less of what we produce.

It's something I notice in my own work. I'll approve an AI-generated draft, and it reads well, and then I realize I'm not sure which ideas are mine and which ones I'm just agreeing with because they sound right.

AI tools are useful. They compress time. They clear blocks. They surface connections I wouldn't have made alone.

But there's a version of this where the tool does the thinking and I do the approving. If I'm honest, that version is seductive. It moves fast. It looks right.

The problem isn't AI. The problem is that fluency is not the same as understanding. Researchers studying AI-assisted work have a term for this: the illusion of fluency. Polished output triggers a sense of comprehension, even when the reader didn't produce the reasoning behind it. Fluent output that I didn't build doesn't represent my judgment, even when I agree with it.

---

This isn't unique to education. A consultant reviewing an AI-drafted deliverable faces it. A manager approving a report their team produced with AI faces it. A writer publishing AI-assisted content faces it.

The question is the same: at what point does the output stop being yours?

There's no bright line. But most of us can feel the difference between directing AI toward a position we've already formed and letting AI form the position while we nod along.

The first is collaboration. The second is something else, and we don't have great language for it yet.

---

I've been working through this for the past year. In my teaching, I noticed I needed different habits than my students did. I was managing a production workflow; they were trying to develop as thinkers.

Same tools, different problems.

That observation sent me into the research: how people think when AI is doing some of the thinking for them, what gets lost, what can be preserved. I've been testing what I've found in my own courses and professional practice.

The concept I keep coming back to is **epistemic stewardship**: owning the thinking behind the work, not just the output. It's not a new term, but it names the gap I keep encountering: the difference between using AI to build on your ideas and using AI instead of having ideas.

I use these tools extensively. This line of thinking developed through using them.

I'll share what I've found over the coming weeks: how the problem looks different depending on whether you already have expertise or are still building it, what specific checkpoints seem to help, and what actually changed when I put this into practice.

---

But I'm more interested in the conversation than the monologue. If you use AI in your professional work, in education or otherwise: how do you know when the work is still yours?

---

*#AIinEducation #EpistemicStewardship #FutureOfWork #AILiteracy*
