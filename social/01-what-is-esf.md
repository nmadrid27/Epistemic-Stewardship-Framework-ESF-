---
title: "What Is Epistemic Stewardship?"
platform: LinkedIn
topic: Framework introduction
status: draft
posted-date:
word-count: ~800
---

# What Is Epistemic Stewardship? (DRAFT)

---

**Post Option A; Problem-First (Recommended)**

I've been watching something happen in my courses, in my own work, and in conversations with colleagues: we're getting better and better at producing content while getting less and less sure it's actually ours.

AI tools are genuinely useful. They compress time. They clear blocks. They surface connections I wouldn't have made alone. But there's a version of this where the tool does the thinking and I do the approving, and if I'm honest, that version is seductive. It moves fast. It looks right.

The problem isn't that AI generates fluent output. The problem is that fluency is not the same as understanding, and fluent output that I didn't build doesn't represent my judgment, even when I agree with it.

This is an epistemic problem, not a productivity problem.

---

Over the past year, I've been developing a framework for what I'm calling **epistemic stewardship**: the practice of maintaining genuine intellectual ownership over work produced in collaboration with AI systems.

It's not an anti-AI stance. I use AI tools extensively, and this framework was developed using those tools. What it addresses is the gap between having AI assistance and being the author of your own work.

The core insight came from my teaching. I noticed that the habits I needed as a faculty member working with AI were fundamentally different from the habits my students needed. I was managing a production workflow: scope, direct, validate, disclose. My students were trying to develop as thinkers and makers, which meant they needed to preserve the productive friction that AI can eliminate.

Those are two different problems. They need two different frameworks.

---

What I built has two levels:

**Level 1 (Content Production):** For faculty, administrators, practitioners: people using AI to produce professional work. Five-phase workflow: Scope → Direct → Build → Validate → Disclose. The center of gravity is the Directive Memo: a human-authored document written before AI is engaged, establishing the intellectual direction of the work. AI executes; the human directs.

**Level 2 (Epistemic Development):** For students: people whose goal isn't just to produce work but to develop their own judgment, voice, and intellectual identity. AI is introduced later in the process, after students have formed their own positions. The emphasis is on Records of Resistance: documentation of what the student chose not to accept, modified, or pushed back on. What you reject teaches you as much as what you keep.

---

The framework is grounded in research on cognitive load, epistemic cognition, metacognitive development, and the emerging literature on human-AI collaboration in creative and academic settings. It doesn't pretend that AI doesn't change how we think: the evidence suggests it does. What it argues is that those changes can be managed with the right practices.

I've integrated it into two courses this spring. Early results are in the "this is uncomfortable and also clarifying" category: which, in my experience, is where genuine learning usually lives.

---

I'm writing this up for publication. But more immediately: if you're a faculty member, instructional designer, or department chair trying to figure out how to take a coherent position on AI in academic work, not just policy, but actual practice; I'd be glad to share what I've found.

The problem is real. The tools to address it exist. We just have to decide to use them.

---

*Hashtags: #HigherEducation #AIinEducation #FacultyDevelopment #AcademicIntegrity #EpistemicStewardship #AILiteracy*

---

**Post Option B; Short Version (300 words)**

We're getting better at producing content and less sure it's actually ours.

AI tools compress time, clear blocks, surface connections. But there's a version of this where the tool does the thinking and you do the approving. That version moves fast. It looks right. And it's easy to lose yourself in it.

I've spent the past year building a framework for what I call **epistemic stewardship**: maintaining genuine intellectual ownership over AI-assisted work.

The core structure has two levels:

**For faculty and practitioners:** A production workflow centered on the Directive Memo: a human-authored document written before AI is engaged, establishing the intellectual direction of the work. You scope the problem, direct the AI, build with it, validate the output, and disclose the collaboration honestly. The sequence matters.

**For students:** A development workflow that preserves the productive friction AI tends to eliminate. Students form positions before AI enters the process. They keep Records of Resistance: documentation of what they pushed back on, modified, or rejected. What you don't accept teaches you as much as what you keep.

These are different problems. A faculty member managing a production workflow is not doing the same thing as a student developing their own judgment and voice. Treating them the same way leads to frameworks that work for neither.

The framework is grounded in research, tested in two courses this spring, and in submission preparation now.

If you're a faculty member, chair, or instructional designer trying to move past "here's our AI policy" and into actual practice: reach out. The tools to do this right exist.

---

*#HigherEducation #AIinEducation #FacultyDevelopment #EpistemicStewardship*
