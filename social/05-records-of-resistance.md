---
title: "What You Rejected Matters"
platform: LinkedIn
topic: Records of Resistance
status: draft
posted-date:
word-count: ~300 (short) / ~420 (mid) / ~550 (long)
---

# What You Rejected Matters (DRAFT)

---

**Post Option A: Short-Form (Recommended for LinkedIn)**

When you work with AI, everyone focuses on what you kept. The output. The final draft. The thing you shipped.

Nobody asks about what you didn't keep.

I think that's backwards. What you rejected from AI output, and why, is where the actual thinking happens.

Every time you read an AI-generated paragraph and decide "no, that's not what I mean," you're exercising judgment. Every time you revise a suggestion because it's close but misses your intent, you're clarifying your own position. Every time you throw out a section because it sounds right but doesn't reflect what you actually believe, that's epistemic work.

One researcher studying AI and learning calls documented rejection "the strongest indicator of genuine ownership." I think that's right. Agreeing with AI is easy. Disagreeing forces you to know what you think.

If you don't track any of that, it disappears. You're left with a final product and no record of the editorial judgment that shaped it. You can't explain your process to anyone, including yourself.

In the epistemic stewardship work I've been developing, I call these records of resistance: a simple log of what you kept, what you revised, and what you rejected, with a brief note on why. Not a lengthy reflection. Just enough to make your thinking visible.

I started keeping them for my own work before I asked students to do it. What surprised me: the "rejected" column is where I learn the most about what I actually think.

The full framework and practical templates are open source. Link in the first comment.

What's the last thing you rejected from an AI draft, and why?

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #AILiteracy*

---

**Post Option B: Mid-Form**

When people talk about AI-assisted work, the conversation is always about what the AI produced. The output. The draft. The thing that shipped.

Nobody talks about what got cut.

I think that's where the real story is. What you rejected from AI output, what you revised, what you looked at and said "no, that's not what I mean," that's where the actual thinking happens.

Every time you read an AI-generated paragraph and decide it doesn't reflect your position, you're exercising editorial judgment. Every time you revise a suggestion because it's close but misses your intent, you're clarifying what you actually mean. Every time you throw out a section that sounds polished but doesn't say what you believe, you're doing epistemic work.

One researcher studying AI and learning calls documented rejection "the strongest indicator of genuine ownership." I think that's right. Agreeing with AI is easy. Disagreeing forces you to know what you think.

But if you don't track any of that, it vanishes. All you're left with is a final product and no record of the decisions that shaped it. You can't explain your process to a colleague, a reviewer, or yourself.

In the epistemic stewardship work I've been developing, I call these **records of resistance**: a simple log of what you kept, what you revised, and what you rejected from AI output, with a brief note on why. Not a lengthy reflection. Just enough to make your editorial judgment visible: three columns and a sentence or two per entry.

I started keeping them for my own professional work before I ever asked students to do it. What surprised me was that the "rejected" column is consistently where I learn the most about what I actually think. Agreeing with AI doesn't teach me anything. Disagreeing with it forces me to articulate why.

The full framework and templates are open source. Link in the first comment.

What's the last thing you rejected from an AI draft, and why?

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #AILiteracy*

---

**Post Option C: Long-Form**

When people talk about AI-assisted work, the conversation is always about what the AI produced. The output. The draft. The thing that shipped.

Nobody talks about what got cut.

I think that's where the real story is. What you rejected from AI output, what you revised, what you looked at and said "no, that's not what I mean," that's where the actual thinking happens.

---

Every time you read an AI-generated paragraph and decide it doesn't reflect your position, you're exercising editorial judgment. Every time you revise a suggestion because it's close but misses your intent, you're clarifying what you actually mean. Every time you throw out a section that sounds polished but doesn't say what you believe, you're doing epistemic work.

But if you don't track any of that, it vanishes. All you're left with is a final product and no record of the decisions that shaped it.

You can't explain your process to a colleague, a reviewer, or yourself. The thinking is invisible.

This is a problem for practitioners, and it's an even bigger problem for students. If a student uses AI to help write a paper and submits only the final version, neither they nor their instructor can see where the learning happened.

The interesting part isn't the polished output. It's the moment the student read the AI's suggestion and thought, "that's not what I think." That's the evidence of engagement.

---

In the epistemic stewardship work I've been developing, I call these **records of resistance**: a simple log of what you kept, what you revised, and what you rejected from AI output, with a brief note on why.

Not a lengthy reflection. Not a formal report. Just enough to make your editorial judgment visible: three columns and a sentence or two per entry.

I started keeping them for my own professional work before I ever asked students to do it. What surprised me was that the "rejected" column is consistently where I learn the most about what I actually think. Agreeing with AI doesn't teach me anything. Disagreeing with it forces me to articulate why.

---

The pattern I keep seeing, in my own work and in my students': people who track what they rejected develop a sharper sense of their own positions over time. People who only track what they used become increasingly uncertain about where their thinking ends and the AI's begins.

The full framework and templates for this are open source. Link in the first comment.

What's the last thing you rejected from an AI draft? I'm curious whether other people find the "no" moments as clarifying as I do.

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #AILiteracy*
