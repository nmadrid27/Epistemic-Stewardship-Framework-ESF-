---
title: "Not All AI-Assisted Work Is the Same Risk"
platform: LinkedIn
topic: Epistemic weight / calibration
status: draft
posted-date:
word-count: ~280 (short) / ~400 (mid) / ~450 (long)
---

# Not All AI-Assisted Work Is the Same Risk (DRAFT)

---

**Post Option A: Short-Form (Recommended for LinkedIn)**

One thing that keeps coming up in conversations about AI use: people treating all AI-assisted work as if it carries the same risk.

It doesn't.

Using AI to format a meeting agenda is not the same as using AI to draft a strategic recommendation. Using AI to compile a resource list is not the same as using AI to write learning outcomes.

The demands are different. The risk of losing ownership is different. The amount of human involvement required is different.

In the epistemic stewardship work I've been developing, I use a simple three-tier classification:

**High weight.** Content where the intellectual position must originate with you. Original arguments, evaluations, strategic recommendations, learning outcomes. Full methodology required.

**Medium weight.** Content where you need to substantively shape the direction and evaluate the result, but AI can contribute to structure and drafting. Syllabi, assignment descriptions, internal reports. Standard workflow with honest disclosure.

**Low weight.** Content where AI can draft with light review. Schedules, formatting, resource lists, routine communications. Quick review, basic attribution.

The classification isn't about how much AI to *allow*. It's about how much human intellectual authority the content *demands*.

Most people I talk to are already doing some version of this intuitively. The value of making it explicit is that intuition drifts.

Research on AI workflow design confirms this: efficiency-oriented systems reward acceptance over scrutiny. The faster you move, the less you notice when medium-weight work starts getting treated as low-weight. High-weight work gets one pass instead of three. Having a classification makes that drift visible.

The full framework and calibration guides are open source. Link in the first comment.

How do you calibrate? Is it intuitive, or do you have a system?

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #Productivity #AILiteracy*

---

**Post Option B: Mid-Form**

A pattern I notice in almost every conversation about AI-assisted work: people treating all of it as if it carries the same risk.

It doesn't. And conflating low-stakes AI use with high-stakes AI use is one of the fastest ways to either burn out on methodology or drift into carelessness.

Using AI to format a meeting agenda is not the same as using AI to draft a strategic recommendation. Using AI to compile a reading list is not the same as using AI to write learning outcomes that define what students are supposed to learn.

The demands are different. The risk of losing ownership is different. The amount of human involvement required is different.

In the epistemic stewardship work I've been developing, I use a simple three-tier classification I call content epistemic weight:

**High weight.** Content where the intellectual position, evaluative judgment, or original argument must originate with you. Thesis statements, assessment criteria, strategic recommendations, original scholarship. Full methodology required: write down your position first, apply the Five Questions at every decision point, document the process.

**Medium weight.** Content where you need to substantively shape the direction and evaluate the result, but AI can contribute to structure and drafting. Syllabi, assignment descriptions, internal strategy documents. Standard workflow with honest disclosure.

**Low weight.** Content where AI can draft with light human review. Scheduling, formatting, resource lists, routine communications. Quick review, basic attribution.

The question isn't how much AI to *allow*. It's how much human intellectual authority the content *demands*. That reframe matters. "How much AI should we allow?" is a policy question with no stable answer. "How much human involvement does this content require?" is an epistemic question with a defensible answer.

Most people I talk to are already calibrating intuitively. The value of making it explicit is that intuition drifts. Research on AI workflow design confirms this: efficiency-oriented systems reward acceptance over scrutiny. The faster you move, the less you notice when medium-weight work starts getting treated as low-weight.

The full framework, including calibration guides and examples, is open source. Link in the first comment.

How do you calibrate? Is it intuitive, or do you have a system?

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #Productivity #AILiteracy*

---

**Post Option C: Long-Form**

A pattern I notice in almost every conversation about AI-assisted work: people treating all of it as if it carries the same risk.

It doesn't. And conflating low-stakes AI use with high-stakes AI use is one of the fastest ways to either burn out on methodology or drift into carelessness.

Using AI to format a meeting agenda is not the same as using AI to draft a strategic recommendation. Using AI to compile a reading list is not the same as using AI to write learning outcomes that define what students are supposed to learn.

The demands are different. The risk of losing ownership is different. The amount of human involvement required is different.

---

In the epistemic stewardship work I've been developing, I use a simple three-tier classification I call content epistemic weight:

**High weight.** Content where the intellectual position, evaluative judgment, or original argument must originate with you. Examples: thesis statements, assessment criteria, strategic recommendations, tenure materials, original scholarship. This demands the full methodology: write down your position first, apply the Five Questions at every decision point, document the process.

**Medium weight.** Content where you need to substantively shape the direction and evaluate the result, but AI can contribute to structure and drafting. Examples: syllabi sections, assignment descriptions, lecture outlines, internal strategy documents. Standard workflow with honest disclosure.

**Low weight.** Content where AI can draft with light human review. Examples: scheduling, formatting, resource lists, routine administrative communications. Quick review, basic attribution.

---

The question isn't how much AI to *allow*. It's how much human intellectual authority the content *demands*.

This reframe matters. "How much AI should we allow?" is a policy question with no stable answer. "How much human involvement does this content require?" is an epistemic question with a defensible answer.

Some content inherently demands more origination from you. That's a property of the content, not a rule someone assigned.

Most people I talk to are already calibrating intuitively. The value of making it explicit is that intuition drifts.

When you're tired or deadline-pressed, medium-weight work starts getting treated as low-weight. High-weight work gets one editing pass instead of three. Having a classification system makes that drift visible before the work goes out the door.

---

The full framework, including calibration guides and examples, is open source. Link in the first comment.

How do you calibrate your own AI use? Gut feeling? Explicit categories? Something else? Curious whether anyone has developed their own system for this.

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #Productivity #AILiteracy*
