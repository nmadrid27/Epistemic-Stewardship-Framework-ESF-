---
title: "Can You Use AI to Build a Framework About Responsible AI Use?"
platform: LinkedIn
topic: Bootstrapping tension
status: draft
posted-date:
word-count: ~300 (short) / ~450 (mid) / ~600 (long)
---

# Can You Use AI to Build a Framework About Responsible AI Use? (DRAFT)

---

**Post Option A: Short-Form (Recommended for LinkedIn)**

The most common question I get about the epistemic stewardship work: "Didn't you use AI to build this?"

Yes. Extensively.

The framework, the implementation guides, the toolkits, the literature review: all developed through sustained human-AI collaboration across months of work.

That's not a contradiction. It's the point.

If a framework for maintaining intellectual ownership in AI-assisted work can't survive contact with AI during its own development, it's not worth publishing. The question isn't whether AI was used. It's whether the intellectual direction, the constructs, the arguments, and the structural decisions originated with a human, and whether that can be demonstrated.

I documented the entire process. Every phase had a directive memo written before AI was engaged. Every transition applied the Five Questions.

The human-AI division of labor is mapped at the session level. Where the process got messy (and it did), I documented that too.

The uncomfortable case: the very first directive memo for the framework was itself assembled by AI from my existing working notes. Those notes were human-authored over months of practice. The AI organized them into memo format.

I reviewed it and confirmed it captured my position. Is that a violation of "write your position before AI enters"? I don't think so, but I documented the tension rather than pretending it didn't exist.

That, to me, is what epistemic stewardship actually looks like. Not perfection. Transparency about where the seams are.

The entire framework, self-application evidence, and process documentation are open source. Link in the first comment. You can read every directive memo and decide for yourself.

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #OpenSource #AILiteracy*

---

**Post Option B: Mid-Form**

The question I get most often about the epistemic stewardship work I've been sharing: "Didn't you use AI to build this?"

Yes. Extensively. Across months of work, dozens of sessions, and every major document in the framework.

And I think that's exactly the right question to ask, because it gets at something important: can a framework for responsible AI use survive its own methodology?

The constructs that became the Epistemic Stewardship Framework didn't start as a framework. They started as scattered solutions to problems I was encountering in my own work: AI-assisted drafts drifting from my intent, students who couldn't tell the difference between their ideas and the AI's, the nagging feeling that "I reviewed it" wasn't the same as "I directed it."

Over months, those solutions accumulated. At some point I realized they formed a coherent system. The formalization process, turning working practices into a publishable framework, is where AI was most heavily involved.

What AI did: drafting, organizing, cross-referencing scholarship, formatting implementation guides, structuring the literature review.

What AI did not do: decide what the framework argues, choose which constructs to include, determine how the two levels relate, or make the judgment calls about what to emphasize and what to set aside.

I can make that distinction because I followed the framework's own protocol during its development. Every phase started with a directive memo I wrote before AI was engaged. Every transition applied the Five Questions. The human-AI division of labor is documented at the session level.

The uncomfortable case: the very first directive memo for the framework was itself assembled by AI from my existing working notes. Those notes were human-authored over months of practice. The AI organized them into memo format. I reviewed it and confirmed it captured my position. Is that a violation of "write your position before AI enters"? I don't think so, but I documented the tension rather than pretending it didn't exist.

That, to me, is what epistemic stewardship actually looks like. Not perfection. Transparency about where the seams are.

The entire framework, self-application evidence, and process documentation are open source. You can read every directive memo and decide for yourself whether the intellectual ownership claim holds.

Not "trust me." Here's the work. Judge it.

Link in the first comment.

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #OpenSource #AILiteracy*

---

**Post Option C: Long-Form**

The question I get most often about the epistemic stewardship work I've been sharing: "Didn't you use AI to build this?"

Yes. Extensively. Across months of work, dozens of sessions, and every major document in the framework.

And I think that's exactly the right question to ask, because it gets at something important: can a framework for responsible AI use survive its own methodology?

---

Here's what the process actually looked like.

The constructs that became the Epistemic Stewardship Framework didn't start as a framework. They started as scattered solutions to problems I was encountering in my own work: AI-assisted drafts drifting from my intent, students who couldn't tell the difference between their ideas and the AI's, the nagging feeling that "I reviewed it" wasn't the same as "I directed it."

Over months, those solutions accumulated: a pre-drafting document here, a set of checkpoint questions there, a classification system for different types of content. At some point I realized they formed a coherent system. The formalization process, turning working practices into a publishable framework, is where AI was most heavily involved.

---

What AI did: drafting, organizing, cross-referencing scholarship, formatting implementation guides, structuring the literature review.

What AI did not do: decide what the framework argues, choose which constructs to include, determine how the two levels relate, or make the judgment calls about what to emphasize and what to set aside.

I can make that distinction because I followed the framework's own protocol during its development. Every phase started with a directive memo I wrote before AI was engaged.

Every transition applied the Five Questions. The human-AI division of labor is documented at the session level with estimated contribution ratios.

---

The uncomfortable part.

The very first directive memo for the framework was itself assembled by AI, from my existing working notes and process documents, all of which were human-authored over months of practice. The AI organized them into memo format. I reviewed the result and confirmed it captured my position accurately.

Is that a violation of "write your position before AI enters the process"? Technically, the memo's format was AI-produced.

But the intellectual content came from months of human-authored working documents. The construct's intent, establishing intellectual authority before drafting begins, was met through the source material.

I documented this tension rather than pretending it didn't exist, because I think that's what honest practice looks like. Epistemic stewardship isn't about never having a messy case. It's about being transparent when you do.

---

The entire framework, the self-application evidence, the directive memos, and the process documentation are open source. You can read every document and decide for yourself whether the intellectual ownership claim holds.

That's the offer. Not "trust me." Here's the work. Judge it.

Link in the first comment.

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #OpenSource #AILiteracy*
