---
title: "The Document Before the Draft"
platform: LinkedIn
topic: Directive Memo
status: draft
posted-date:
word-count: ~280 (short) / ~400 (mid) / ~500 (long)
---

# The Document Before the Draft (DRAFT)

---

**Post Option A: Short-Form (Recommended for LinkedIn)**

Try this the next time you use AI to write something that matters.

Before you open the tool, write down what you actually think. Not a prompt. Not an outline. Just: what is this work arguing? What matters most? What are you deliberately setting aside? What is non-negotiable?

It takes five minutes. And it shifts what happens next.

There's research backing this up. Studies on AI-mediated workflows show that without a pre-drafting step, people shift from reflective planning to immediate interaction with the tool. The AI becomes the starting point, not the assistant.

Once AI gives you a well-structured draft, your own position becomes very hard to separate from the AI's framing. You read the output and think, "that's basically what I was going to say."

But you don't actually know that, because you never wrote down what you were going to say. The AI's version replaced it before you could compare.

This is the problem I keep encountering in my own work on epistemic stewardship. The moment of highest risk isn't when AI generates something wrong. It's when AI generates something plausible that you didn't originate, and you adopt it because it sounds like you.

Writing down your position first, before the AI is involved, gives you something to compare against. It turns "I agree with this" into "this aligns with what I wrote, or it doesn't." That's a different kind of evaluation.

I've been calling this a directive memo in the framework I'm developing, but the name doesn't matter. What matters is the sequence: think first, then delegate. Not the other way around.

I've open-sourced the full framework, guides, and toolkits I'm building around this. Link in the first comment.

Do you write anything down before you start working with AI? Or do you go straight to the tool and evaluate as you go?

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #AILiteracy*

---

**Post Option B: Mid-Form**

Something I've noticed in my own AI-assisted work, and I think it's universal:

Once AI gives you a well-structured draft, your own position becomes almost impossible to separate from the AI's framing.

You read the output and think, "that's basically what I was going to say." But you don't actually know that. You never wrote down what you were going to say. The AI's version arrived first, and now it's the frame you're evaluating against, not the other way around.

This is not an intelligence problem. It's a sequencing problem. And it happens to everyone, regardless of expertise.

There's research backing this up. Studies on AI-mediated workflows show that without a pre-drafting step, people shift from reflective planning to immediate interaction with the tool. The AI becomes the starting point, not the assistant.

The simplest intervention I've found is also the most uncomfortable: write down what you think before you open the tool.

Not a prompt. Not an outline for the AI to follow. A document for yourself: What is this work actually arguing? What matters most? What am I deliberately setting aside? What's non-negotiable?

It takes five minutes. And it creates something you can't create after the fact: an uncontaminated record of your own thinking.

Once you have that, evaluating AI output changes. "I agree with this" becomes "this aligns with what I wrote, or it doesn't." That's not a subtle distinction. It's the difference between evaluation and retroactive endorsement.

In the framework I'm developing, I call this a directive memo. But the name is less important than the practice. The sequence matters: position first, then delegation. Not the reverse.

I've open-sourced the full framework, guides, and toolkits I'm building around this. Link in the first comment.

Do you write anything down before you start working with AI? Or do you go straight to the tool and evaluate as you go?

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #AILiteracy*

---

**Post Option C: Long-Form**

Something I've noticed in my own AI-assisted work, and I think it's universal:

Once AI gives you a well-structured draft, your own position becomes almost impossible to separate from the AI's framing.

You read the output and think, "that's basically what I was going to say." But you don't actually know that. You never wrote down what you were going to say. The AI's version arrived first, and now it's the frame you're evaluating against, not the other way around.

This is not an intelligence problem. It's a sequencing problem. And it happens to everyone, regardless of expertise.

---

I've been thinking about this a lot in my work on epistemic stewardship, and the simplest intervention I've found is also the most uncomfortable: write down what you think before you open the tool.

Not a prompt. Not an outline for the AI to follow. A document for yourself: What is this work actually arguing? What matters most? What am I deliberately setting aside? What's non-negotiable?

---

It takes five minutes. And it creates something you can't create after the fact: an uncontaminated record of your own thinking.

Once you have that, evaluating AI output changes. "I agree with this" becomes "this aligns with what I wrote, or it doesn't."

That's not a subtle distinction. It's the difference between evaluation and retroactive endorsement.

In the framework I'm developing, I call this a directive memo. But the name is less important than the practice. The sequence matters: position first, then delegation. Not the reverse.

---

I've been open-sourcing the full framework, implementation guides, and practitioner toolkits as I develop them. If you want to see what this looks like in practice, the link is in the first comment.

I'm curious: do you write anything down before you start working with AI? Or do you go straight to the tool and evaluate on the fly? If you do write something first, what does it look like?

---

*First comment: GitHub repo link*

*#EpistemicStewardship #AIinEducation #FutureOfWork #AILiteracy*
