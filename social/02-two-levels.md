---
title: "Two Different Problems"
platform: LinkedIn
topic: Two-Level Architecture
status: draft
posted-date:
word-count: ~280 (short) / ~420 (mid) / ~550 (long)
---

# Two Different Problems (DRAFT)

---

**Post Option A: Short-Form (Recommended for LinkedIn)**

Something I keep running into: AI policies that treat everyone as if they have the same problem.

An experienced professional using AI to draft a report already has expertise. They can evaluate what the AI gives them. Their question: *Is this good enough to put into the world?*

A student, or a junior employee producing their first strategy memo with AI, is in a different situation. They're building judgment they don't yet fully have. Their question: *Do I actually understand what I'm doing?*

One is a production problem. The other is a development problem. Most organizations are writing one policy for both.

Recent scholarship on AI in education keeps arriving at this same split. One researcher frames it as a three-tiered scaffold: foundational thinking first, then augmented synthesis, then full integration. The stages require fundamentally different support.

Here's where it breaks: "Make sure the work reflects your understanding" is reasonable advice for someone with deep expertise. For someone still developing that expertise, it assumes the very capacity they're trying to build.

I've been wrestling with what it looks like to treat these as separate problems. Same underlying principle (the human stays the author of their thinking), but different approaches for different situations.

Does your organization distinguish between these two situations? I'm curious how others are handling this, whether in education or industry.

---

*#AIinEducation #HigherEducation #EpistemicStewardship #FutureOfWork #AIPolicy*

---

**Post Option B: Mid-Form**

Something I keep running into, in conversations with colleagues across education and industry: AI policies that treat everyone as if they have the same relationship to AI output.

They don't.

A senior professional using AI to draft a report is managing a production workflow. They already have expertise. They can evaluate what the AI gives them against what they know. The question they need to answer: *Is this good enough to put into the world?*

A student using AI to write an essay, or a junior employee producing their first strategy memo with AI, is in a fundamentally different situation. They're not managing production. They're developing the capacity to think, to form positions, to exercise judgment they don't yet fully possess. The question they need to answer: *Do I actually understand what I'm doing?*

One is a production problem. The other is a development problem. Most organizations are writing one policy for both.

Recent scholarship on AI in education keeps arriving at this same split. One researcher frames it as a three-tiered scaffold: foundational thinking first, then augmented synthesis, then full integration. The stages require fundamentally different support.

Here's where it breaks: "Make sure the work reflects your understanding" sounds reasonable. For someone with deep domain expertise, it is. For someone still building that baseline, it assumes the very capacity they're trying to develop. You can't use it as a prerequisite for the process that builds it.

I've been wrestling with this as part of a broader concept: **epistemic stewardship**, owning the thinking behind AI-assisted work, not just the output. The split between production and development keeps showing up as the central tension.

When we treat these as one problem, we get policies that are either too permissive for people who need structure to develop their own judgment, or too restrictive for practitioners who need latitude to work efficiently.

Does your organization distinguish between these two situations?

---

*#AIinEducation #HigherEducation #EpistemicStewardship #FutureOfWork #AIPolicy*

---

**Post Option C: Long-Form**

Something I keep running into, in conversations with colleagues across education and industry: AI policies that treat everyone as if they have the same relationship to AI output.

They don't.

A senior professional using AI to draft a report is managing a production workflow. They already have expertise. They can evaluate what the AI gives them against what they know. The question they need to answer: *Is this good enough to put into the world?*

A student using AI to write an essay, or a junior employee using AI to produce their first strategy memo, is in a fundamentally different situation. They're not managing production. They're developing the capacity to think, to form positions, to exercise judgment they don't yet fully possess. The question they need to answer: *Do I actually understand what I'm doing?*

One is a production problem. The other is a development problem. Most organizations, in education and in industry, are writing one policy for both.

---

Here's where it breaks down.

"Disclose your AI use and make sure the work reflects your understanding" sounds reasonable. For someone with deep domain expertise, it is. They can evaluate AI output against a baseline they've spent years building.

For someone still building that baseline, the same instruction is nearly meaningless. "Make sure the work reflects your understanding" assumes you can already distinguish between your understanding and a fluent approximation of it. That's exactly the capacity you're trying to develop. You can't use it as a prerequisite for the process that builds it.

---

I've been wrestling with this as part of a broader concept: **epistemic stewardship**: owning the thinking behind AI-assisted work, not just the output. The split between production and development keeps showing up as the central tension. In a university, it's the difference between a faculty member using AI for course development and a student using it for a term paper. In a company, it's the difference between a VP producing a board presentation and a new hire producing their first client deliverable.

When we treat these as one problem, we get policies that are either too permissive for people who need more structure to develop their own judgment, or too restrictive for practitioners who need latitude to work efficiently.

I don't think I've fully solved this. But I'm increasingly convinced that any organization trying to write a single AI policy for everyone is building something that serves no one well.

Is anyone else seeing this split? How is your institution or organization handling the difference between people who already have expertise and people who are still building it?

---

*#AIinEducation #FacultyDevelopment #EpistemicStewardship #FutureOfWork #AIPolicy*
