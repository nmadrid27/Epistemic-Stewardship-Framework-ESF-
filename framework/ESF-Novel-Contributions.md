---
title: "ESF Novel Contributions: Gap Analysis with Evidence"
author: Nathan Madrid
date: 2026-02-19
version: "2.0-draft"
status: draft
type: novel-contributions
epistemic-weight: high
parent-document: ESF-Framework-Document.md
tags: [esf, novel-contributions, gap-analysis]
---

# ESF Novel Contributions

## Gap Analysis with Evidence

---

## Introduction

The Epistemic Stewardship Framework makes six claims of novel contribution to the literature on AI in higher education. This document presents each claim in a form structured for peer review defensibility: the claim itself, evidence that the gap exists, how ESF addresses it, anticipated counterarguments, and responses. The goal is intellectual honesty: positioning ESF's contributions precisely against what already exists, not overstating its originality.

These contributions are scoped to the AI-in-education and AI-assisted academic work literature as it stands in early 2026. Analogues may exist in adjacent fields: composition pedagogy, design research methodology, and reflective practice, and cross-domain comparison with those traditions is both warranted and an explicit direction for future work.

---

## 1. The Directive Memo: Pre-Drafting Intellectual Authority

### Claim

No existing framework in the AI-in-education literature requires a human-only pre-drafting phase in which the author establishes intellectual authority (including thesis, emphasis, de-emphasis, rationale, voice, and non-negotiables) before any AI-assisted production begins. Adjacent fields, particularly composition pedagogy and writing instruction, have related traditions of pre-writing stance and writer's memo practices.

### Evidence of Gap

**AIAS** (Perkins et al., 2024) defines five levels of AI involvement in student assessment. It specifies *how much* AI is permitted for a given task but does not address the *process* by which the human establishes intellectual direction before AI engagement. An instructor can set the AIAS level for an assignment without having articulated what the assignment should argue or emphasize.

**AID** (Weaver, 2024) structures disclosure after production is complete. It provides templates for documenting AI involvement but does not intervene in the production process itself. A user can follow AID protocols perfectly, disclosing every AI contribution, while having exercised no pre-drafting intellectual authority.

**UNESCO** (Miao & Holmes, 2023) and the **OECD** (2023) advocate for human agency in AI-assisted work as a principle. Neither specifies a mechanism for asserting that agency at the point in the workflow where it matters most: before AI-assisted production begins.

**Clark and Chalmers** (1998) define the conditions for legitimate cognitive extension, including active endorsement, but do not operationalize these conditions for any specific domain. The philosophical foundation exists. The practical mechanism does not.

### How ESF Fills It

The Directive Memo operationalizes Clark and Chalmers's active endorsement condition by requiring the human to articulate their intellectual position before the AI provides its synthesis. This temporal ordering is the key innovation. Once AI produces a fluent, well-structured draft, the human's own position becomes harder to distinguish from the AI's framing (Tankelevitch et al., 2024). The memo captures human intellectual direction in its uncontaminated form, before the AI's output exists to influence it.

The memo also creates the standard against which the Integrity Report evaluates the final work: did the output preserve the human's stated direction, or did it drift?

### Anticipated Counterarguments

**"Good practitioners already do this informally."** Some faculty do sketch their ideas before engaging AI. The counterargument is that informal, unenforced practice is not a framework contribution. The gap is structural: no framework *requires* it, documents it, or uses it as the evaluative standard for the completed work. The Directive Memo formalizes a practice that may occur informally; formalizing it makes it teachable, assessable, and institutionally enforceable.

**"A prompt is essentially a directive memo."** A prompt instructs the AI on what to produce. A Directive Memo declares the human's intellectual position. A prompt may say "write a syllabus for an AI ethics course." A Directive Memo says "this course argues that ethics is a design constraint, not a philosophical addendum; emphasize studio critique over regulatory analysis; de-emphasize the standard trolley-problem framing; the voice should be provocative; the assigned Dunne & Raby reading is non-negotiable." The distinction is between operational instruction and intellectual authority.

**"The memo adds bureaucratic overhead without proportional value."** This concern is legitimate for low-stakes content, which is why ESF calibrates the memo requirement to Content Epistemic Weight. Low-weight content (schedules, formatting) does not require a memo. The overhead applies only to content where intellectual ownership genuinely matters. For high-weight content (scholarship, accreditation reports, assessment design), the memo takes minutes to write and provides the only available pre-drafting standard against which the final work's integrity can be assessed.

---

## 2. Calibrated Epistemic Weight as Framework Logic

### Claim

ESF reframes content-level AI involvement from a policy choice (how much AI should be *allowed*?) to an epistemic property (how much human intellectual authority does this content *demand*?).

### Evidence of Gap

**AIAS** (Perkins et al., 2024) provides the closest precedent with its five-level AI involvement scale. AIAS is a substantial contribution: it replaces binary policies with graduated options. However, AIAS frames these levels as instructor-assigned policy decisions for student assessments. The instructor decides whether an assignment permits Level 2 (AI-assisted ideation) or Level 4 (AI-led generation with human editing). The framing is administrative: the policy determines the level.

**Bloom et al.** (1956) provide the cognitive taxonomy that maps to human involvement demands, but Bloom's taxonomy predates AI and does not address AI-specific calibration.

No existing framework treats the required depth of human involvement as an *inherent property of the content type*, independent of policy, applicable to all users, and derived from the cognitive demands of the content rather than from an administrative decision.

### How ESF Fills It

Content Epistemic Weight classifies content types into three tiers (High / Medium / Low) based on the inherent epistemic demands of the content. A tenure portfolio is high-weight not because a policy says so but because the intellectual claims it contains must originate with the author: their arguments, their evidence, their contribution. A meeting agenda is low-weight not because a policy permits AI assistance but because its content does not make intellectual claims that require human origination.

This reframing is more durable than policy-based calibration. As AI capabilities change, policies must be continuously revised ("now AI can do X, should we still require Y?"). Epistemic weight is more stable: the cognitive demands of a tenure portfolio do not change because AI becomes more capable. What changes is *how* the human meets those demands, not *whether* they must be met.

### Anticipated Counterarguments

**"The distinction between policy-based and property-based calibration is semantic."** A reviewer might argue that whether we call it "AI involvement level" (AIAS) or "epistemic weight" (ESF), the result is the same: a classification that determines how much human effort is required. The response: the framing matters because it determines who holds the authority. AIAS levels are set by the instructor for student work; they are governance tools. ESF's epistemic weight is assessed by any user for any content; it is an epistemic analysis. A faculty member using ESF classifies their own work's epistemic weight, not just their students'. This self-application is both a practical difference and a philosophical one.

**"Three tiers is too coarse; AIAS's five levels provide more nuance."** This is a fair critique. ESF's response is that the three-tier model prioritizes practical adoption: users can classify content quickly (is this high, medium, or low?) and know what workflow components to apply. Five or more levels increase classification overhead without proportional gains in practice, particularly for faculty who are not assessment specialists. Institutions adapting ESF could subdivide the tiers if more granularity serves their context.

**"Epistemic weight may vary by discipline; what's high-weight in humanities might be low-weight in engineering."** The tiers are defined by cognitive demand, not disciplinary convention. A thesis statement is high-weight in any discipline because it requires the author to commit to an intellectual position. A bibliography is low-weight in any discipline because it compiles existing references. The *content* within each tier is discipline-specific; the *tier logic* is universal.

---

## 3. Iterative Methodology with Epistemic Accountability

### Claim

ESF bridges two literatures, iterative design methodology and epistemic accountability in AI-assisted work, that have not been systematically connected in the AI-in-education literature.

### Evidence of Gap

**Iterative design methodologies** (Brown, 2008; Torrance, 2019) provide phase structures with human decision points at convergences. They do not embed epistemic accountability mechanisms. The question at each convergence is "is this good?" not "is this mine?"

**AI-in-education frameworks** (AIAS, AID, UNESCO) provide assessment, disclosure, and policy models. They do not define the iterative production process that precedes assessment or disclosure. They evaluate outcomes; they do not structure the production process.

The result is a gap: practitioners have iterative methods for producing work and separate frameworks for ensuring integrity, but no integrated methodology that does both.

### How ESF Fills It

The five-phase workflow model (Scope, Direct, Build, Validate, Disclose) embeds Human Validation Gates with the Five Questions at every phase transition. This is not iterative design *followed by* integrity review. It is iterative design *with* epistemic accountability at every iteration. The Directive Memo provides the stable reference point that iterative revision is measured against, preventing the common failure mode in which iterative processes gradually drift from their original intent.

### Anticipated Counterarguments

**"Design thinking already includes human judgment at convergence points; ESF just adds checkboxes."** The counterargument distinguishes between *quality* judgment (is this good enough?) and *epistemic* judgment (is this mine?). A design thinking convergence can pass with full marks even if the human has passively accepted every AI suggestion; the product is good, so the convergence succeeds. An ESF gate fails in that scenario because the Five Questions expose the passive acceptance.

**"The integration is obvious; anyone using iterative methods with AI would naturally add integrity checks."** If this were true, the literature would show examples. It does not. Torrance (2019) and Brown (2008) do not address AI; Perkins et al. (2024), Weaver (2024), and UNESCO (2023) do not provide iterative production methods. The integration may seem obvious in retrospect. Many useful frameworks do. The contribution is making it explicit, systematic, and replicable.

---

## 4. Faculty-Facing Methodology

### Claim

ESF provides the first systematic methodology for faculty use of AI in their own professional academic work (curriculum development, assessment design, scholarship, administrative reporting) as opposed to faculty implementation of student-facing AI frameworks.

### Evidence of Gap

**Zawacki-Richter et al. (2019)** documented this gap in their systematic review: the overwhelming majority of AI-in-education research targets student-facing applications. Faculty appear as implementers and instructors, not as practitioners who use AI in their own work.

Since their review, the literature has expanded, but the expansion has largely maintained the asymmetry:
- **Allen & Kendeou (2024)**: student AI literacy
- **Perkins et al. (2024)**: student assessment
- **Kassorla et al. (2024)**: institutional strategies for student AI competencies
- **Weaver (2024)**: disclosure (applicable to faculty but not faculty-specific)
- **UNESCO (2023), OECD (2023)**: policy-level guidance

Faculty remain the intended *users* of these frameworks but not their *subjects.*

### How ESF Fills It

The ESF Faculty Implementation Guide (forthcoming) translates the framework's constructs into discipline-agnostic workflows for common faculty tasks. More importantly, ESF positions faculty practice as the foundation for student instruction: the framework argues that credible AI-integrated teaching requires faculty to practice epistemic stewardship in their own work first.

This is not merely an expansion of the target audience. It changes the framework's logic: faculty are not implementing a student-facing tool. They are practicing a methodology that applies to their own work, and the student-facing application follows from their own experience as practitioners.

### Anticipated Counterarguments

**"Faculty are experts; they don't need structured protocols for AI use."** Tankelevitch et al. (2024) demonstrate that expertise fails to protect against the metacognitive failures AI-assisted work creates. Even experienced professionals struggle to detect when AI output subtly departs from their own understanding, particularly when the output is fluent and well-structured. The argument that expertise suffices is empirically challenged.

**"Faculty AI use is too diverse to be captured in a single methodology."** ESF does not prescribe specific practices for specific tasks. It provides a structural methodology (the five-phase workflow with gates) that adapts to any task through the Content Epistemic Weight classification. A faculty member developing a syllabus, writing a grant proposal, or drafting an accreditation report follows the same structural methodology, with the Directive Memo and Integrity Report calibrated to the task's specific demands.

**"Faculty will resist additional process requirements."** This is a practical concern, not a scholarly one, and a legitimate one. ESF addresses it through the weight model: not every task requires the full workflow. Low-weight tasks require only attribution. Medium-weight tasks require abbreviated protocols. The full five-phase workflow with Directive Memo and Integrity Report applies only to high-weight content, where intellectual ownership genuinely matters and the overhead is justified by the stakes.

---

## 5. The Five Questions as Structured Decision Gates

### Claim

ESF introduces structured, recurring epistemic decision gates: five specific, answerable questions applied at defined workflow transitions with explicit halt conditions. This mechanism has no precedent in the AI-in-education literature (as with all ESF claims, scoped to that literature; adjacent fields such as facilitation methodology, coaching practice, and Socratic questioning traditions employ structured question sets for reflection and decision-making).

### Evidence of Gap

Existing frameworks include:
- **Reflection prompts**: UNESCO (2023) and OECD (2023) encourage reflection on AI use
- **Ethical guidelines**: Allen and Kendeou (2024) specify AI literacy competencies
- **Assessment criteria**: Perkins et al. (2024) define levels of acceptable AI involvement
- **Disclosure requirements**: Weaver (2024) structures transparency

None specifies a set of answerable (yes/no) questions applied at defined points in a production workflow, with the explicit condition that a "no" answer halts the process.

The gap is between *principles* (reflect on AI use, be transparent, maintain ownership) and *mechanisms* (here are five specific questions, asked at these specific points, and a "no" stops the work).

### How ESF Fills It

The Five Questions target specific metacognitive failure modes documented in the literature:

1. **Can I defend this?** Targets passive acceptance of plausible-sounding content. If the user cannot explain the work without referencing the AI's reasoning, they have not achieved the understanding the content claims.
2. **Is this mine?** Targets the substitution of AI framing for human narrative direction. Tests whether the human directed the intellectual argument or adopted the AI's framing. Directly operationalizes Clark and Chalmers's active endorsement condition.
3. **Did I verify?** Targets verification neglect. AI-generated citations, data, and factual claims require independent confirmation.
4. **Would I teach this?** Targets shallow endorsement. Standing behind content in a professional context (classroom, committee, review board) demands deeper engagement than approving it in a private review.
5. **Is the disclosure honest?** Targets performative disclosure. If the disclosure statement does not accurately represent the division of intellectual labor, the entire transparency mechanism fails.

The questions recur at every Human Validation Gate, counteracting the cognitive automation (Atchley et al., 2024) that degrades oversight quality over time. Their yes/no structure produces a definitive result: the workflow proceeds or it halts. This operational specificity distinguishes the Five Questions from philosophical principles that recommend reflection without structuring it.

### Anticipated Counterarguments

**"Five questions are arbitrary; why not three, or seven?"** Each question targets a documented failure mode. The set could be expanded if new failure modes are identified (and the Framework Evolution Protocol provides the mechanism for that). The current five were derived from operational practice and supported by the literature:

- *Can I defend this?* targets passive acceptance (Tankelevitch et al., 2024)
- *Is this mine?* targets epistemic drift (Wu et al., 2025)
- *Did I verify?* targets verification neglect (Stoyanov, 2026)
- *Would I teach this?* targets shallow endorsement (Tankelevitch et al., 2024)
- *Is the disclosure honest?* targets performative disclosure (Weaver, 2024)

The number reflects the failure modes, not an arbitrary count.

**"Repeated questions become performative; users will say 'yes' automatically."** This is the cognitive automation concern (Atchley et al., 2024) applied to the framework itself, and it is a legitimate risk. ESF mitigates it through the Gate Verification Record: at each gate, the user documents not just their answers but what they reviewed, changed, and challenged. A pattern of unqualified "yes" answers with empty verification records is itself a signal of performative engagement. The Five Questions are not a perfect mechanism (no metacognitive intervention is), but they are a structural intervention where the literature currently offers only aspirational guidance.

**"The halt condition is impractical; deadlines don't allow indefinite pauses."** ESF does not require indefinite pauses. A "no" answer means the issue must be resolved before proceeding, which may take minutes (re-reading a section more carefully) or hours (rewriting a section that has drifted from the Directive Memo). The halt condition is practical precisely because it applies at phase transitions, not at every sentence. The overhead is proportional to the severity of the epistemic concern.

---

## 6. Two-Level Architecture: Distinct Process Models for Producers and Learners

### Claim

ESF is the only framework that provides structurally distinct but parallel process models for content production (faculty, administrators, institutional leaders) and epistemic development (students), recognizing that these serve fundamentally different purposes within a single integrated architecture.

### Evidence of Gap

The literature separates into two streams that do not converge.

**Production-oriented work** is addressed by frameworks designed for practitioners who create content. AIAS calibrates AI involvement for assessment design, AID structures disclosure for completed work, and UNESCO provides policy guidance for institutional governance. None of these provides a structured production workflow. All address faculty as implementers or regulators rather than as practitioners with their own epistemic responsibilities.

**Student-facing scaffolding** is addressed by a growing body of research (see Literature Review, B.8) that establishes how student engagement should be structured (Hutson, 2025; Washington State OSPI, 2024; Pasmala et al., 2026; Degen & Asanov, 2025). These contributions do not connect student engagement processes to the faculty's own production process.

**The gap between streams.** No existing framework recognizes that the faculty member designing a syllabus (content production) and the student completing an assignment from that syllabus (epistemic development) are engaged in fundamentally different epistemic activities that require different process models. Existing approaches either give both audiences the same framework at different intensities (treating students as simplified faculty) or address them in separate, unconnected publications.

### How ESF Fills It

ESF's two-level architecture provides:

**Level 1 (Content Production):** The five-phase workflow (Scope, Direct, Build, Validate, Disclose) for faculty, administrators, and institutional leaders. Gates ask: *Is this good enough to put into the world?* The Directive Memo captures professional judgment. The Integrity Report documents epistemic process. The purpose is production quality and intellectual ownership.

**Level 2 (Epistemic Development):** A distinct five-phase process (Inquire, Position, Explore, Make, Reflect) for students. AI does not enter until Phase 3. Phases 1 and 2 are human-only, requiring students to articulate their understanding and establish their position before AI output exists to react to. Gates ask: *Do I actually understand what I'm doing?* The Position Statement replaces the Directive Memo, focusing on the student's own thinking rather than professional authority. The purpose is learning and epistemic agency development.

**The structural connection.** The levels are not parallel tracks. They are architecturally linked: a faculty member using Level 1 to design an assignment is simultaneously designing the epistemic environment a student works within at Level 2. The Directive Memo for assignment design (Level 1) shapes the conditions under which students practice the Inquire-Position-Explore-Make-Reflect process (Level 2). This connection ensures that faculty who require epistemic discipline from students are practicing it themselves.

### Anticipated Counterarguments

**"This is just a faculty guide and a student guide, not a novel contribution."** The claim is not that ESF has two guides. The claim is that ESF provides two structurally distinct process models, with different phases, different gate questions, and different purposes, that are architecturally linked within a single framework. Treating them as the same thing (applying one model at different intensities) is the design flaw the two-level architecture addresses.

**"Other frameworks already distinguish faculty and student audiences."** They do, but through separate publication tracks or audience-specific guidance, not through an integrated architecture with distinct process models. AIAS provides assessment levels for student work but no production methodology for faculty work. ED-AI Lit provides competencies for students but no methodology for how faculty develop their own AI practices. The distinction ESF draws is structural: different processes for different epistemic activities, connected by architectural design.

**"The student process model is just design thinking relabeled."** The phase names (Inquire, Position, Explore, Make, Reflect) do parallel design thinking (Empathize, Define, Ideate, Prototype, Test). The structural innovation is what happens at each phase: human-only Phases 1-2 before AI access, Socratic human gates rather than quality gates, process artifacts that document records of resistance (what was rejected from AI, not just what was accepted), and progressive scaffolding across four engagement levels. Design thinking does not include any of these mechanisms.

---

## Summary

| Contribution | Precedent Status | Strongest Evidence of Gap | Strongest Counterargument |
|-------------|-----------------|--------------------------|--------------------------|
| Directive Memo | No precedent within AI-in-education literature | No framework requires pre-drafting intellectual authority mechanism; adjacent fields (composition pedagogy) have related practices; cross-domain comparison is future work | "Good practitioners do this informally," but informal does not equal systematic |
| Epistemic Weight as Property | Partial (AIAS) | AIAS frames as policy; ESF frames as inherent property | "The distinction is semantic," but framing determines who holds authority |
| Iterative + Epistemic | No precedent | Design lit and epistemic lit are disconnected | "Integration is obvious," but the literature doesn't show it |
| Faculty-Facing | Documented gap | Zawacki-Richter (2019) + no subsequent closure | "Faculty are experts," but expertise does not equal metacognitive immunity |
| Five Questions as Gates | No precedent | Principles exist; mechanisms do not | "Repeated questions become performative," mitigated by Gate Verification Record |
| Two-Level Architecture | No precedent | Production and scaffolding literatures are disconnected; no framework provides distinct process models for both | "Just two guides," but distinct process models with architectural linkage are not separate documents |

---

> **AI Collaboration Disclosure:** This gap analysis was developed through human-AI collaboration. The identification of novel contributions, their positioning against existing literature, and the assessment of counterargument strength reflect the author's analysis. AI assisted with structural organization, prose drafting, and cross-claim consistency. This document follows the ESF protocol.

---

*Version 2.0-draft | 2026-02-19*
*Epistemic Stewardship Framework; Novel Contributions*
*Nathan Madrid*
